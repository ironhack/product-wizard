Title: Lazy Fetch for RAG Context (Lightweight State)

Goal
- Store only lightweight references (file names/IDs) in state; fetch snippets on demand right before generation/validation/expansion.
- Reduce steady-state and peak memory on eco dynos without altering RAG behavior.

Scope
- Retrieval: persist only sources (filenames) and selected_file_ids. Do NOT persist full chunk texts.
- Generation/Validation/Expansion: use a helper to fetch fresh snippets limited by caps, based on sources/IDs.
- State hygiene: clear any temporary text fields at end of turn.

Design
1) Retrieval Node (retrieve_documents)
   - Current: saves sources, selected_file_ids, retrieved_docs (text), evidence_chunks (text).
   - Change: save only sources, selected_file_ids. Do not save retrieved_docs/evidence_chunks (or keep them empty lists).

2) Helper: fetch_snippets_for_sources(sources: List[str], query: str, per_doc: int, per_snippet_chars: int, total_chars: int) -> List[(source, snippet)]
   - Implementation: call OpenAI Responses with file_search and hint line "FOCUS_FILES: <comma-separated sources>" built from sources.
   - Filter results to only the requested sources, collect up to `per_doc` snippets per source.
   - Truncate each snippet to `per_snippet_chars` and stop when `total_chars` is reached.
   - Return list of (source, snippet) preserving order.

3) Generation (generate_response)
   - Before composing system context, call fetch_snippets_for_sources with parameters derived from env caps (e.g., per_doc=1â€“2, per_snippet_chars=RAG_PER_DOC_CHARS, total_chars=RAG_GENERATION_TOTAL_CHARS).
   - Build context by joining returned snippets.
   - Keep sources for citation detection. Do not store snippets back into state.

4) Validation (validate_response)
   - Call fetch_snippets_for_sources with stricter caps (e.g., per_doc=1, smaller total_chars) to build evidence_text.
   - Proceed with current validation flow.

5) Expansion (expand_document_chunks)
   - Call fetch_snippets_for_sources with per_doc=RAG_EXPANSION_CHUNKS_PER_DOC and possibly a modified query suffix.
   - Set state.sources/selected_file_ids as is; set a temporary in-function snippets list only for immediate retry generation.
   - Option A (preferred): pass snippets forward via a transient key (e.g., transient_snippets) consumed by generate_response and then dropped.
   - Option B: call fetch again inside generate_response when retry_expansion=True.

6) State Cleanup (finalize_response)
   - Ensure no text blobs remain in state. Keep only: messages, sources, selected_file_ids, counters/flags.

7) Env Caps (already present)
   - RAG_TOP_FILES, RAG_PER_DOC_CHARS, RAG_GENERATION_TOTAL_CHARS, RAG_EXPANSION_CHUNKS_PER_DOC, RAG_MAX_CONTEXT_TOKENS.

Trade-offs
- Pros: Near-zero memory retention; lower peak RAM; safer on eco dynos.
- Cons: +1 OpenAI call for generation and possibly for validation; slight latency/cost increase.

Rollout Plan
1) Implement helper function and wire into generate/validate/expand as described.
2) Feature-flag: add LAZY_FETCH_ENABLED=true env var; fallback to current behavior if false.
3) Test locally: python tests/master_test.py --tests all
4) Validate latency impact; adjust caps if needed.
5) Enable in production by flipping env var.

Backout Plan
- Toggle LAZY_FETCH_ENABLED=false to restore current capped-in-state behavior.

Notes
- Keep logging minimal for snippet arrays; never log full texts.
- Ensure dedupe and Slack flow unchanged; only data plumbing changes.
