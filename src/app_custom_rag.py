"""
Custom RAG Slack App with Hybrid Architecture
- Responses API for retrieval + grounded generation with citations
- Chat Completions API for JSON validators (variant QA, comparisons)
- Slack integration and conversation context
- Variant-aware answers (only: remote, berlin)
- Program tokens grounded to your docs (Portfolio/Design/Certifications 2025_07)
- No hardcoded content claims; all answers must come from retrieved docs
"""

from slack_bolt import App
from slack_bolt.adapter.flask import SlackRequestHandler
from flask import Flask, request
import openai
import os
import logging
import re
import time
import json
import tempfile
import fcntl
from typing import Dict, List, Tuple, Optional

# ---------------- Logging ----------------
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

SAFE_FALLBACK_MSG = (
    "I don't have complete information about this topic in our curriculum materials. "
    "Please reach out to the Education team on Slack for the specific details you need."
)

# ---------------- Slack init ----------------
SLACK_BOT_TOKEN = os.environ.get("SLACK_BOT_TOKEN", "")
SLACK_SIGNING_SECRET = os.environ.get("SLACK_SIGNING_SECRET", "")

slack_app = None
handler = None
try:
    if SLACK_BOT_TOKEN and SLACK_SIGNING_SECRET:
        slack_app = App(token=SLACK_BOT_TOKEN, signing_secret=SLACK_SIGNING_SECRET)
        handler = SlackRequestHandler(slack_app)
        logger.info("Slack app initialized successfully")
    else:
        logger.info("Slack env vars missing. Running without Slack handlers.")
except Exception as e:
    logger.error(f"Failed to initialize Slack app: {e}")
    slack_app = None
    handler = None

# ---------------- OpenAI init ----------------
client = openai.OpenAI(api_key=os.environ["OPENAI_API_KEY"])
VECTOR_STORE_ID = os.environ.get("OPENAI_VECTOR_STORE_ID", "vs_xxx")

# ---------------- Config loaders ----------------
def load_config_file(filename):
    try:
        script_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        file_path = os.path.join(script_dir, 'assistant_config', filename)
        with open(file_path, 'r', encoding="utf-8") as f:
            return f.read()
    except Exception as e:
        logger.info(f"Config {filename} not found. Using defaults. Detail: {e}")
        return ""

def load_master_prompt():
    return load_config_file('MASTER_PROMPT.md') or (
        "You are a helpful assistant for Ironhack course information. "
        "Answer only from provided documents. "
        "Never invent facts. "
        "If multiple variants exist, never blend content between variants."
    )

MASTER_PROMPT = load_master_prompt()
GENERATION_INSTRUCTIONS = load_config_file('GENERATION_INSTRUCTIONS.md')
VALIDATION_INSTRUCTIONS = load_config_file('VALIDATION_INSTRUCTIONS.md')
RETRIEVAL_INSTRUCTIONS = load_config_file('RETRIEVAL_DEFAULT.md')

# ---------------- Tokens grounded to your docs ----------------
# Variants: ONLY remote and berlin (as per your corpus)
VARIANT_TOKENS = ("remote", "berlin", "onsite", "online")

# Programs discovered in:
# /mnt/data/Ironhack_Portfolio_Overview_2025_07.md (#### headings with acronyms)
# We derive aliases strictly from those headings + their acronyms; no extra nicknames.
PROGRAM_TOKENS: Dict[str, List[str]] = {
    "web development": [
        "web development", "web_development", "web dev", "web_dev", "webdev", "wdft", "wdpt", "wd"
    ],
    "ux/ui design": [
        "ux/ui design", "ux_ui_design", "uxui", "uxft", "uxpt", "ux"
    ],
    "data analytics": [
        "data analytics", "data_analytics", "dataanalytics", "daft", "dapt", "da"
    ],
    "data science & machine learning": [
        "data science & machine learning",
        "data science and machine learning",
        "data_science_and_machine_learning",
        "data_science_machine_learning",
        "data_science_&_machine_learning",
        "data science & ml", "data_science_&_ml", "ds_ml",
        "mlft", "mlpt", "dsml", "ml"
    ],
    "ai engineering": [
        "ai engineering", "ai_engineering", "ai eng", "ai_eng", "aieng", "aift", "aipt", "ai"
    ],
    "devops": [
        "devops", "dev ops", "dev_ops", "dvft", "dvpt", "dv"
    ],
    "marketing": [
        "marketing", "mktg", "mkft", "mkpt", "mk"
    ],
    "cybersecurity": [
        "cybersecurity", "cyber security", "cyber_security", "cysec", "cyft", "cypt", "cy"
    ],
    "data science and ai 1-year program germany": [
        "data science and ai 1-year program germany",
        "DSAI", "DF1Y"
    ],
    "intensive program in applied ai - ai async productivity course": [
        "intensive program in applied ai - ai async productivity course",
        "ai async productivity course", "apac"
    ],
}

CERT_TOKENS = ["certification", "certifications", "certificate", "certificates", "cert", "badge", "exam", "accreditation"]
COVERAGE_TOKENS = ["does", "include", "cover", "teach", "have", "is there", "do you cover", "is it covered"]
HARDWARE_TOKENS = [
    "hardware", "computer", "laptop", "mac", "windows", "linux",
    "specs", "specifications", "requirements", "minimum requirements",
    "ram", "cpu", "m1", "disk space", "admin permissions", "administrator",
    "8gb", "i5", "i3", "screen size", "13\"", "2015 or newer"
]

# ---------------- RAG Pipeline ----------------
class CustomRAGPipeline:
    TOP_K_FILES = 4
    MAX_NUM_RESULTS = 40
    MIN_SCORE = 0.0

    def __init__(self, client, vector_store_id, master_prompt):
        self.client = client
        self.vector_store_id = vector_store_id
        self.master_prompt = master_prompt
        self._selected_file_ids: List[str] = []
        self._id_to_filename: Dict[str, str] = {}
        self._evidence_chunks: List[Dict] = []

    # ---------- dynamic safe fallback ----------
    def generate_safe_fallback(self, query: str, reason: str = "") -> str:
        try:
            system_msg = (
                "You generate a single short fallback message when the assistant cannot answer "
                "from the retrieved curriculum documents. The message must: (1) politely explain "
                "we're not answering to avoid inventing facts, (2) be lightly friendly/playful, "
                "optionally referencing not making Rudy upset about fabrications, (3) invite the "
                "user to contact the Education team on Slack, (4) be at most 2 sentences, (5) no "
                "document citations or sources section, (6) no promises or speculative numbers, "
                "(7) respond in the same language as the user's query; if language is unclear, "
                "default to English. If the query is in English, include the exact phrase: "
                "'reach out to the Education team on Slack'."
            )
            user_msg = (
                f"Query: {query or ''}\n"
                f"Reason: {reason or 'unspecified'}\n"
                "Instructions: Respond in the same language as the query; if unsure, use English. "
                "If the query is in English, include exactly: 'reach out to the Education team on Slack'."
            )
            resp = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_msg},
                    {"role": "user", "content": user_msg},
                ],
                temperature=0.6,
                max_tokens=120,
            )
            text = (resp.choices[0].message.content or "").strip()
            # Guardrails: ensure no Sources block sneaks in
            text = re.sub(r"\n+Sources:\n(?:- .+\n?)+$", "", text, flags=re.IGNORECASE | re.MULTILINE).strip()
            if not text:
                return SAFE_FALLBACK_MSG
            return text
        except Exception:
            return SAFE_FALLBACK_MSG

    # ---------- helpers ----------
    def _normalize_filename(self, filename: str) -> str:
        if not filename:
            return ""
        base = filename.rsplit("/", 1)[-1]
        base = base.rsplit(".", 1)[0]
        return base.replace("_", " ").strip()

    def _contains_any(self, text: str, tokens: List[str]) -> bool:
        t = (text or "").lower()
        return any(tok in t for tok in tokens)

    def _program_from_query(self, query: str) -> str:
        q = (query or "").lower()
        for program, toks in PROGRAM_TOKENS.items():
            if any(tok in q for tok in toks):
                return program
        return ""

    def _is_cert_query(self, query: str) -> bool:
        return self._contains_any(query, CERT_TOKENS)

    def _is_coverage_query(self, query: str) -> bool:
        return self._contains_any(query, COVERAGE_TOKENS)

    def _is_hardware_query(self, query: str) -> bool:
        return self._contains_any(query, HARDWARE_TOKENS)

    def _get_retrieval_instructions(self, query):
        # Use the consolidated retrieval instructions for all query types
        # The automated system handles context-aware, comparison, and overview queries internally
        return (RETRIEVAL_INSTRUCTIONS or "").strip()

    def _enhance_query_with_context(self, query, conversation_context):
        if not conversation_context:
            return query
        context_info = []
        mentioned_programs = set()
        last_program = ""
        last_variant = ""
        last_intent = ""

        duration_tokens = ['how long', 'duration', 'hours', 'weeks', 'length', 'part-time', 'full-time']
        # steer retrieval when conversation is about hardware requirements
        hardware_tokens = HARDWARE_TOKENS

        # Look further back for better reference resolution
        for msg in conversation_context[-6:]:
            role = msg.get('role')
            content = (msg.get('content') or "")
            lower = content.lower()
            if role == 'user':
                for program, toks in PROGRAM_TOKENS.items():
                    if any(t in lower for t in toks):
                        mentioned_programs.add(program)
                        last_program = program
                for vt in VARIANT_TOKENS:
                    if vt in lower:
                        last_variant = vt
                if any(tok in lower for tok in duration_tokens):
                    last_intent = 'duration'
                elif self._is_cert_query(lower):
                    last_intent = 'certifications'
                elif self._is_coverage_query(lower):
                    last_intent = 'coverage'
                elif self._is_hardware_query(lower):
                    last_intent = 'hardware'
                context_info.append(f"Previously discussed: {content}")
            elif role == 'assistant':
                if any(term in lower for term in ['bootcamp', 'program', 'certification', 'course']):
                    first_sentence = content.split('.', 1)[0].strip()
                    if first_sentence:
                        context_info.append(f"Previous response mentioned: {first_sentence}")
                    for program, toks in PROGRAM_TOKENS.items():
                        if any(t in lower for t in toks):
                            mentioned_programs.add(program)
                            last_program = program
                    for vt in VARIANT_TOKENS:
                        if vt in lower:
                            last_variant = vt
                    if any(tok in lower for tok in duration_tokens):
                        last_intent = 'duration'
                    elif self._is_cert_query(lower):
                        last_intent = 'certifications'
                    elif self._is_coverage_query(lower):
                        last_intent = 'coverage'
                    elif self._is_hardware_query(lower):
                        last_intent = 'hardware'

        qlower = (query or "").lower()
        # Strengthen pronoun/ellipsis resolution for references like "that bootcamp"
        if any(ref in qlower for ref in ['that bootcamp', 'that program', 'that', 'this', 'it', 'they', 'them']):
            if last_program:
                referring = f"Referring to: {last_program}"
                if last_variant:
                    referring += f" ({last_variant})"
                context_info.insert(0, referring)
            elif mentioned_programs:
                context_info.insert(0, f"Referring to: {', '.join(sorted(mentioned_programs))}")

        # If the intent appears to be duration, make that explicit to guide retrieval
        if any(tok in qlower for tok in duration_tokens) or last_intent == 'duration':
            context_info.append("Intent: duration (seek hours/weeks) for the referred program")

        if any(tok in qlower for tok in hardware_tokens) or last_intent == 'hardware':
            context_info.append("Intent: hardware requirements (seek minimum computer specs)")

        if context_info:
            return f"{query} (Context: {' '.join(context_info)})"
        return query

    def _variant_label_from_name(self, name: str) -> str:
        n = (name or "").lower()
        if "remote" in n:
            return "remote"
        if "berlin" in n:
            return "berlin"
        return "unspecified"

    def _by_variant(self) -> Dict[str, List[Tuple[str, str]]]:
        byv = {}
        for fid, fname in self._id_to_filename.items():
            v = self._variant_label_from_name(fname)
            byv.setdefault(v, []).append((fid, fname))
        return byv

    def _variants_present_for_selected(self) -> List[str]:
        variants = set()
        id_to_name = self._id_to_filename or {}
        for fid in getattr(self, "_selected_file_ids", []) or []:
            fname = id_to_name.get(fid, "")
            v = self._variant_label_from_name(fname)
            if v in ("remote", "berlin"):
                variants.add(v)
        return sorted(list(variants))

    def _ensure_single_sources_block(self, text: str, filenames: List[str]) -> str:
        text = text or ""
        # strip existing Sources blocks
        text = re.sub(r'\n+Sources:\n(?:- .+\n?)+$', '', text, flags=re.IGNORECASE | re.MULTILINE)
        if filenames:
            bullets = "\n".join(f"- {self._normalize_filename(fn)}" for fn in filenames)
            text = f"{text}\n\nSources:\n{bullets}"
        return text.strip()

    # ---------- retrieval ----------
    def retrieve_documents(self, query, conversation_context=None):
        try:
            self._evidence_chunks = []
            enhanced_query = self._enhance_query_with_context(query, conversation_context)
            instructions = self._get_retrieval_instructions(enhanced_query)

            resp = self.client.responses.create(
                model="gpt-4o-mini",
                input=[{"role": "user", "content": enhanced_query}],
                instructions=instructions,
                tools=[{
                    "type": "file_search",
                    "vector_store_ids": [self.vector_store_id],
                    "max_num_results": self.MAX_NUM_RESULTS
                }],
                tool_choice={"type": "file_search"},
                include=["file_search_call.results"]
            )

            hits = []
            for out in getattr(resp, "output", []):
                # responses includes results directly or within file_search_call
                res = getattr(out, "results", None)
                if res:
                    hits = res
                    break
                fsc = getattr(out, "file_search_call", None)
                if fsc:
                    if getattr(fsc, "results", None):
                        hits = fsc.results
                        break
                    if getattr(fsc, "search_results", None):
                        hits = fsc.search_results
                        break

            if not hits:
                # fallback to older endpoint
                try:
                    vs = self.client.vector_stores.search(
                        vector_store_id=self.vector_store_id,
                        query=enhanced_query
                    )
                    hits = getattr(vs, "data", []) or []
                except Exception as e:
                    logger.error(f"vector_stores.search fallback failed: {e}")

            if not hits:
                self._selected_file_ids = []
                self._id_to_filename = {}
                return [], []

            by_file: Dict[str, Dict] = {}
            id_to_filename = {}

            for r in hits:
                fname = getattr(r, "filename", None) or getattr(getattr(r, "document", None), "filename", None)
                fid = getattr(r, "file_id", None) or getattr(getattr(r, "document", None), "id", None)
                score = float(getattr(r, "score", 0.0) or 0.0)

                text = ""
                if hasattr(r, "text") and r.text:
                    text = r.text
                else:
                    parts = getattr(r, "content", []) or []
                    if parts and hasattr(parts[0], "text"):
                        text = parts[0].text
                    else:
                        content = getattr(getattr(r, "document", None), "content", None)
                        if content:
                            text = content

                if not fname or not fid or not text:
                    continue
                if score < self.MIN_SCORE:
                    continue

                id_to_filename[fid] = fname
                entry = by_file.get(fname)
                if entry is None or score > entry["score"]:
                    by_file[fname] = {"text": text, "score": score, "file_id": fid}

            if not by_file:
                self._selected_file_ids = []
                self._id_to_filename = {}
                return [], []

            # Prefer files whose filename clearly matches the user's target program, to reduce cross-program noise
            q_program = self._program_from_query(enhanced_query)
            def filename_matches_program(fname: str, program_key: str) -> bool:
                if not program_key:
                    return False
                name_lower = (fname or "").lower()
                tokens = PROGRAM_TOKENS.get(program_key, [])
                return any(tok in name_lower for tok in tokens)

            sorted_all = sorted(by_file.items(), key=lambda kv: kv[1]["score"], reverse=True)
            if q_program:
                preferred = [it for it in sorted_all if filename_matches_program(it[0], q_program)]
                others = [it for it in sorted_all if it not in preferred]
                ranked = preferred + others
            else:
                ranked = sorted_all

            top = ranked[: self.TOP_K_FILES]

            sources = []
            retrieved_content = []
            selected_file_ids = []
            for fname, entry in top:
                sources.append(fname)
                retrieved_content.append(entry["text"])
                selected_file_ids.append(entry["file_id"])

            self._selected_file_ids = selected_file_ids
            id_map = {entry["file_id"]: fname for fname, entry in top}
            id_map.update(id_to_filename)
            self._id_to_filename = id_map

            return retrieved_content, sources

        except Exception as e:
            logger.error(f"Error retrieving documents: {e}", exc_info=True)
            self._selected_file_ids = []
            self._id_to_filename = {}
            return [], []

    # ---------- generation with citations ----------
    def _create_temp_store(self, file_ids):
        vs = self.client.vector_stores.create(
            name="tmp_rag_run",
            expires_after={"anchor": "last_active_at", "days": 1}
        )
        for fid in file_ids:
            try:
                self.client.vector_stores.files.create(vector_store_id=vs.id, file_id=fid)
            except Exception as e:
                logger.error(f"Error attaching file {fid} to temp store: {e}")
        return vs

    def _wait_until_indexed(self, vs_id, expected_count, timeout_s=30, poll_s=0.5):
        import time as _t
        start = _t.time()
        while _t.time() - start < timeout_s:
            try:
                vs = self.client.vector_stores.retrieve(vs_id)
                fc = getattr(vs, "file_counts", None)
                if fc and getattr(fc, "completed", 0) >= expected_count and getattr(fc, "in_progress", 0) == 0:
                    return True
            except Exception:
                pass
            _t.sleep(poll_s)
        return False

    def _extract_evidence_chunks(self, resp):
        chunks = []
        for out in getattr(resp, "output", []):
            res = None
            if hasattr(out, "results") and out.results:
                res = out.results
            else:
                fsc = getattr(out, "file_search_call", None)
                if fsc and getattr(fsc, "results", None):
                    res = fsc.results
                elif fsc and getattr(fsc, "search_results", None):
                    res = fsc.search_results
            if not res:
                continue
            for r in res:
                fname = getattr(r, "filename", None)
                fid = getattr(r, "file_id", None)
                text = ""
                if hasattr(r, "text") and r.text:
                    text = r.text
                else:
                    parts = getattr(r, "content", []) or []
                    if parts and hasattr(parts[0], "text"):
                        text = parts[0].text
                if fname and fid and text:
                    chunks.append({"file_id": fid, "filename": fname, "text": text})

        # dedupe
        seen = set()
        deduped = []
        for c in chunks:
            key = (c["file_id"], c["text"])
            if key not in seen:
                seen.add(key)
                deduped.append(c)
        return deduped

    def _extract_citations_from_responses(self, resp):
        id_to_filename = dict(self._id_to_filename)
        for out in getattr(resp, "output", []):
            fsc = getattr(out, "file_search_call", None)
            res = None
            if hasattr(out, "results") and out.results:
                res = out.results
            elif fsc and getattr(fsc, "results", None):
                res = fsc.results
            elif fsc and getattr(fsc, "search_results", None):
                res = fsc.search_results
            if res:
                for r in res:
                    fid = getattr(r, "file_id", None)
                    fname = getattr(r, "filename", None)
                    if fid and fname:
                        id_to_filename[fid] = fname

        used = []
        for out in getattr(resp, "output", []):
            for part in getattr(out, "content", []):
                annotations = getattr(part, "annotations", []) or []
                for ann in annotations:
                    if getattr(ann, "type", "") == "file_citation":
                        fid = getattr(ann, "file_id", None)
                        fname = getattr(ann, "filename", None)
                        if fid:
                            used.append(fname or id_to_filename.get(fid, fid))

        # dedupe preserving order
        seen = set()
        used_unique = []
        for u in used:
            if u and u not in seen:
                used_unique.append(u)
                seen.add(u)
        return used_unique

    def generate_response(self, query, retrieved_docs, conversation_context=None, sources=None):
        try:
            sel_ids = getattr(self, "_selected_file_ids", []) or []
            if not sel_ids:
                return self.generate_response_fallback(query, retrieved_docs, conversation_context, sources)

            vs = self._create_temp_store(sel_ids)
            ok = self._wait_until_indexed(vs.id, expected_count=len(sel_ids))
            if not ok:
                return self.generate_response_fallback(query, retrieved_docs, conversation_context, sources)

            msgs = [{"role": "system", "content": self.master_prompt}]
            # Enforce side-by-side output when both variants are present
            variants_present = self._variants_present_for_selected()
            if "remote" in variants_present and "berlin" in variants_present:
                msgs.append({
                    "role": "system",
                    "content": (
                        "Both Remote and Berlin variants are present in the retrieved sources. "
                        "Provide a concise, side-by-side answer with separate sections titled 'Remote' and 'Berlin'. "
                        "Include a brief quote and filename per variant. Do not describe only one variant, and never blend content."
                    )
                })
            # General generation instructions from config already include coverage guidance
            if conversation_context:
                for m in conversation_context[-6:]:
                    role = "user" if m["role"] == "user" else "assistant"
                    content = m["content"][:800] if len(m["content"]) > 800 else m["content"]
                    msgs.append({"role": role, "content": content})
            msgs.append({"role": "user", "content": query})

            resp = self.client.responses.create(
                model="gpt-4o",
                input=msgs,
                tools=[{"type": "file_search", "vector_store_ids": [vs.id], "max_num_results": 20}],
                tool_choice={"type": "file_search"},
                include=["file_search_call.results"]
            )

            self._evidence_chunks = self._extract_evidence_chunks(resp)
            answer = getattr(resp, "output_text", "") or ""
            used_filenames = self._extract_citations_from_responses(resp)

            try:
                self.client.vector_stores.delete(vs.id)
            except Exception:
                pass

            return self._ensure_single_sources_block(answer, used_filenames)

        except Exception as e:
            logger.error(f"Error generating response: {e}", exc_info=True)
            return self.generate_safe_fallback(query, reason="generation_error")

    def generate_response_fallback(self, query, retrieved_docs, conversation_context=None, sources=None):
        try:
            context = "\n\n".join(retrieved_docs) if retrieved_docs else "No documents retrieved"
            source_info = "\n".join([f"- {s}" for s in (sources or [])]) if sources else "No specific sources"

            system_prompt = f"""{self.master_prompt}

RETRIEVED CONTEXT:
{context}

SOURCE INFORMATION:
{source_info}

{GENERATION_INSTRUCTIONS}
"""

            # Enforce side-by-side output when both variants are present (fallback path)
            variants_present = self._variants_present_for_selected()
            if "remote" in variants_present and "berlin" in variants_present:
                system_prompt += (
                    "\n\nIMPORTANT: Both Remote and Berlin variants are present. "
                    "Provide two clear sections ('Remote' and 'Berlin') with concise differences and cite the filename per variant."
                )
            # Coverage-specific guidance for fallback path
            if self._is_coverage_query(query):
                system_prompt += (
                    "\n\nFor coverage questions: quote a short line if the topic is explicitly mentioned. "
                    "If a variant has no explicit mention, say 'The [Variant] syllabus does not list <topic> in the retrieved document' and cite the filename. "
                    "Avoid asserting 'does not cover' without an explicit statement."
                )

            messages = [{"role": "system", "content": system_prompt}]
            if conversation_context:
                for msg in conversation_context[-6:]:
                    role = "user" if msg["role"] == "user" else "assistant"
                    content = msg["content"]
                    if len(content) > 800:
                        content = content[:800] + "..."
                    messages.append({"role": role, "content": content})
            messages.append({"role": "user", "content": query})

            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=messages,
                temperature=0.3
            )
            generated = response.choices[0].message.content
            if sources and len(sources) > 0 and "Sources:" not in generated:
                generated = self._ensure_single_sources_block(generated, sources)
            return generated
        except Exception as e:
            logger.error(f"Error generating fallback response: {e}", exc_info=True)
            return self.generate_safe_fallback(query, reason="fallback_error")

    # ---------- validation ----------
    def _strip_sources_section(self, text):
        if not text:
            return text
        m = re.search(r'\nSources:\n(?:- .+\n?)+$', text, flags=re.IGNORECASE | re.MULTILINE)
        if m:
            return text[:m.start()].strip()
        return text.strip()

    def validate_response(self, response, retrieved_docs):
        try:
            core_resp = self._strip_sources_section(response)
            if getattr(self, "_evidence_chunks", None):
                evidence_text = "\n\n".join(c["text"] for c in self._evidence_chunks)
            else:
                evidence_text = "\n\n".join(retrieved_docs) if retrieved_docs else ""

            if not evidence_text.strip():
                return {
                    "contains_only_retrieved_info": True,
                    "unsupported_claims": [],
                    "confidence": 1.0,
                    "explanation": "No evidence available. Treated as safe."
                }

            validation_instructions = f"""{VALIDATION_INSTRUCTIONS}

EVIDENCE TEXT:
{evidence_text}

RESPONSE TO VALIDATE:
{core_resp}
"""

            v = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "Return only valid JSON with the requested keys. No prose."},
                    {"role": "user", "content": validation_instructions}
                ],
                temperature=0,
                response_format={"type": "json_object"}
            )

            result_raw = json.loads(v.choices[0].message.content.strip())
            result = {
                "contains_only_retrieved_info": bool(result_raw.get("contains_only_retrieved_info", False)),
                "unsupported_claims": [str(x) for x in result_raw.get("unsupported_claims", [])],
                "confidence": float(result_raw.get("confidence", 0)),
                "explanation": str(result_raw.get("explanation", "")),
            }

            # softening rule
            if (not result["contains_only_retrieved_info"] and
                len(result["unsupported_claims"]) == 1 and
                result["confidence"] <= 0.6):
                result["contains_only_retrieved_info"] = True
                result["unsupported_claims"] = []
                result["explanation"] = "Single low confidence nit ignored."
                result["confidence"] = 0.9

            return result

        except Exception as e:
            logger.error(f"Error validating response: {e}", exc_info=True)
            return {
                "contains_only_retrieved_info": True,
                "unsupported_claims": [],
                "confidence": 1.0,
                "explanation": "Validator error. Treated as supported."
            }

    # ---------- main ----------
    def process_query(self, query, conversation_context=None):
        start_time = time.time()

        retrieved_docs, sources = self.retrieve_documents(query, conversation_context)

        if not retrieved_docs:
            total_time = time.time() - start_time
            validation = {
                "contains_only_retrieved_info": True,
                "unsupported_claims": [],
                "confidence": 1.0,
                "explanation": "Skipped validation because no documents were retrieved and safe fallback was used."
            }
            return {
                "response": self.generate_safe_fallback(query, reason="no_documents"),
                "retrieved_docs_count": 0,
                "sources": sources,
                "validation": validation,
                "processing_time": total_time
            }

        response = self.generate_response(query, retrieved_docs, conversation_context, sources)
        pre_validation_response = response
        validation = self.validate_response(response, retrieved_docs)

        confidence_threshold = 0.6
        if (validation.get('confidence', 0) < confidence_threshold or
            not validation.get('contains_only_retrieved_info', False)):
            response = self.generate_safe_fallback(query, reason="validation_failure")
            post_validation = {
                "contains_only_retrieved_info": True,
                "unsupported_claims": [],
                "confidence": 1.0,
                "explanation": "Used safe fallback due to validation failure."
            }
        else:
            post_validation = dict(validation)

        total_time = time.time() - start_time
        result_payload = {
            "response": response,
            "retrieved_docs_count": len(retrieved_docs),
            "sources": sources,
            "validation": post_validation,
            # debug fields for testers
            "pre_validation_response": pre_validation_response,
            "pre_validation": validation,
            "processing_time": total_time
        }
        return result_payload

# ---------------- Init pipeline ----------------
custom_rag = CustomRAGPipeline(client, VECTOR_STORE_ID, MASTER_PROMPT)

# ---------------- Dedup cache ----------------
class MessageDeduplicationCache:
    def __init__(self, max_size=1000, ttl_seconds=300):
        self.max_size = max_size
        self.ttl_seconds = ttl_seconds
        self.cache_file = os.path.join(tempfile.gettempdir(), 'message_cache_custom_rag.json')

    def load_cache(self):
        try:
            if os.path.exists(self.cache_file):
                with open(self.cache_file, 'r') as f:
                    fcntl.flock(f.fileno(), fcntl.LOCK_SH)
                    try:
                        return json.load(f)
                    finally:
                        fcntl.flock(f.fileno(), fcntl.LOCK_UN)
        except Exception as e:
            logger.error(f"Error loading message cache: {str(e)}")
        return {}

    def is_duplicate(self, message_id):
        cache = self.load_cache()
        current_time = time.time()
        cache = {k: v for k, v in cache.items() if current_time - v < self.ttl_seconds}
        if message_id in cache:
            return True
        cache[message_id] = current_time
        try:
            with open(self.cache_file, 'w') as f:
                fcntl.flock(f.fileno(), fcntl.LOCK_EX)
                try:
                    json.dump(cache, f)
                finally:
                    fcntl.flock(f.fileno(), fcntl.LOCK_UN)
        except Exception as e:
            logger.error(f"Error saving message cache: {str(e)}")
        return False

dedup_cache = MessageDeduplicationCache()

# ---------------- Conversation store ----------------
def _mapping_path():
    return os.path.join(tempfile.gettempdir(), 'conversation_mapping_custom_rag.json')

def load_conversation_mapping():
    try:
        mapping_file = _mapping_path()
        if os.path.exists(mapping_file):
            with open(mapping_file, 'r') as f:
                fcntl.flock(f.fileno(), fcntl.LOCK_SH)
                try:
                    return json.load(f)
                finally:
                    fcntl.flock(f.fileno(), fcntl.LOCK_UN)
    except Exception as e:
        logger.error(f"Error loading conversation mapping: {str(e)}")
    return {}

def save_conversation_mapping(mapping):
    try:
        mapping_file = _mapping_path()
        with open(mapping_file, 'w') as f:
            fcntl.flock(f.fileno(), fcntl.LOCK_EX)
            try:
                json.dump(mapping, f)
            finally:
                fcntl.flock(f.fileno(), fcntl.LOCK_UN)
    except Exception as e:
        logger.error(f"Error saving conversation mapping: {str(e)}")

def update_conversation_mapping(conversation_id, conversation_data):
    mapping = load_conversation_mapping()
    mapping[conversation_id] = conversation_data
    save_conversation_mapping(mapping)
    return mapping

def add_message_to_conversation(conversation_id, role, content, timestamp=None):
    if timestamp is None:
        timestamp = time.time()
    mapping = load_conversation_mapping()
    if conversation_id not in mapping:
        mapping[conversation_id] = {"messages": []}
    mapping[conversation_id]["messages"].append({
        "role": role,
        "content": content,
        "timestamp": timestamp
    })
    if len(mapping[conversation_id]["messages"]) > 12:
        mapping[conversation_id]["messages"] = mapping[conversation_id]["messages"][-12:]
    save_conversation_mapping(mapping)
    return mapping

def get_conversation_context(conversation_id):
    mapping = load_conversation_mapping()
    if conversation_id not in mapping or not mapping[conversation_id].get("messages"):
        return []
    messages = mapping[conversation_id]["messages"]
    return messages[-8:] if len(messages) > 8 else messages

# ---------------- Text utils ----------------
def clean_citations(text):
    text = re.sub(r'\[([^\]]+)\]\([^)]+\)', r'\1', text)  # [text](url) -> text
    text = re.sub(r'\[([^\]]+)\]', r'\1', text)           # [text] -> text
    return text

def convert_markdown_to_slack(text):
    text = re.sub(r'\*\*(.*?)\*\*', r'*\1*', text)  # bold
    text = re.sub(r'(?<!\*)\*([^*]+)\*(?!\*)', r'_\1_', text)  # italics
    text = re.sub(r'^#+\s*(.+)$', r'*\1*', text, flags=re.MULTILINE)  # headers
    text = re.sub(r'^-\s+', '• ', text, flags=re.MULTILINE)  # bullets
    return text

# ---------------- Slack handlers ----------------
def process_message(event, say):
    user_message = event.get('text', '')

    message_id = event.get('ts', str(time.time()))
    if dedup_cache.is_duplicate(message_id):
        logger.info(f"Duplicate message detected: {message_id}")
        return

    conversation_id = event.get('thread_ts', event.get('channel'))
    logger.info(f"Processing message in conversation: {conversation_id}")
    logger.info(f"User message: {user_message}")

    current_conversation_mapping = load_conversation_mapping()
    if conversation_id not in current_conversation_mapping:
        current_conversation_mapping = update_conversation_mapping(conversation_id, {"messages": []})

    try:
        conversation_context = get_conversation_context(conversation_id)
        logger.info("Processing with Custom RAG Pipeline...")
        rag_result = custom_rag.process_query(user_message, conversation_context)
        assistant_message = rag_result["response"]

        logger.info("RAG Results:")
        logger.info(f"   Retrieved docs: {rag_result['retrieved_docs_count']}")
        logger.info(f"   Sources: {rag_result['sources']}")
        logger.info(f"   Validation confidence: {rag_result['validation'].get('confidence', 0):.2f}")
        logger.info(f"   Processing time: {rag_result['processing_time']:.2f}s")

        add_message_to_conversation(conversation_id, "user", user_message)
        add_message_to_conversation(conversation_id, "assistant", assistant_message)

    except Exception as e:
        logger.error(f"Error processing message with Custom RAG: {str(e)}")
        try:
            say("I'm having trouble accessing our course information right now. Please reach out to the Education team on Slack for the details you need.")
        except Exception as e2:
            logger.error(f"Error sending failure response: {str(e2)}")
        return

    cleaned_message = clean_citations(assistant_message)
    slack_message = convert_markdown_to_slack(cleaned_message)

    try:
        say(slack_message, thread_ts=event.get('ts'))
        logger.info("Response sent successfully")
    except Exception as e:
        logger.error(f"Error sending response: {str(e)}")
        try:
            say("I'm having trouble sending my response right now. Please try again, or reach out to the Education team on Slack.")
        except Exception as e2:
            logger.error(f"Error sending backup response: {str(e2)}")

# Register handlers only if Slack is initialized
if slack_app is not None:
    @slack_app.event("app_mention")
    def handle_mention(event, say):
        process_message(event, say)

    @slack_app.event("message")
    def handle_message(event, say):
        if event.get("channel_type") == "im":
            process_message(event, say)

# ---------------- Flask app ----------------
flask_app = Flask(__name__)

@flask_app.route("/slack/events", methods=["POST"])
def slack_events():
    if handler:
        try:
            return handler.handle(request)
        except Exception as e:
            logger.error(f"Error handling Slack event: {e}")
            return {"error": f"Error processing Slack event: {str(e)}"}, 500
    else:
        logger.error("Slack handler not initialized - check environment variables")
        return {"error": "Slack handler not initialized - check SLACK_BOT_TOKEN and SLACK_SIGNING_SECRET"}, 500

@flask_app.route("/health", methods=["GET"])
def health_check():
    return {
        "status": "healthy",
        "api": "custom_rag",
        "pipeline": "responses_retrieval + chat_completions_generation",
        "validation": "enabled",
        "variants_supported": list(VARIANT_TOKENS),
        "programs_grounded": list(PROGRAM_TOKENS.keys()),
    }

if __name__ == "__main__":
    flask_app.run(debug=True, host='0.0.0.0', port=int(os.environ.get('PORT', 5000)))