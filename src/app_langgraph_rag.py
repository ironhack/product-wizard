"""
Simplified LangGraph-based Custom RAG Slack App
- Trusts LLM for context understanding (no manual token enhancement)
- Uses LangGraph's built-in memory for conversation persistence
- Streamlined workflow: retrieve → generate → validate → respond
"""

import os
import logging
import re
import json
import time
from typing import Dict, List, TypedDict, Annotated, Sequence
from operator import add

from slack_bolt import App
from slack_bolt.adapter.flask import SlackRequestHandler
from flask import Flask, request

import openai
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver

# ---------------- Logging ----------------
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ---------------- Environment Setup ----------------
SLACK_BOT_TOKEN = os.environ.get("SLACK_BOT_TOKEN", "")
SLACK_SIGNING_SECRET = os.environ.get("SLACK_SIGNING_SECRET", "")
OPENAI_API_KEY = os.environ["OPENAI_API_KEY"]
VECTOR_STORE_ID = os.environ.get("OPENAI_VECTOR_STORE_ID", "vs_xxx")

# Initialize OpenAI client
openai_client = openai.OpenAI(api_key=OPENAI_API_KEY)

# ---------------- Config Loaders ----------------
def load_config_file(filename):
    try:
        script_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        file_path = os.path.join(script_dir, 'assistant_config', filename)
        with open(file_path, 'r', encoding="utf-8") as f:
            return f.read()
    except Exception as e:
        logger.info(f"Config {filename} not found. Using defaults. Detail: {e}")
        return ""

# Load configuration
MASTER_PROMPT = load_config_file('MASTER_PROMPT.md') or (
    "You are a helpful assistant for Ironhack course information. "
    "Answer only from provided documents. Never invent facts."
)
GENERATION_INSTRUCTIONS = load_config_file('GENERATION_INSTRUCTIONS.md')
VALIDATION_INSTRUCTIONS = load_config_file('VALIDATION_INSTRUCTIONS.md')
RETRIEVAL_INSTRUCTIONS = load_config_file('RETRIEVAL_INSTRUCTIONS.md')
DOCUMENT_FILTERING_INSTRUCTIONS = load_config_file('DOCUMENT_FILTERING_INSTRUCTIONS.md')

# New externalized configs
FALLBACK_CLASSIFIER_PROMPT = load_config_file('FALLBACK_CLASSIFIER.md')
FUN_FALLBACK_TEMPLATES = load_config_file('FUN_FALLBACK_TEMPLATES.md')
EXPANSION_INSTRUCTIONS_TEXT = load_config_file('EXPANSION_INSTRUCTIONS.md')
TEAM_ROUTING_RULES_TEXT = load_config_file('TEAM_ROUTING_RULES.md')
COVERAGE_CLASSIFICATION_PROMPT = load_config_file('COVERAGE_CLASSIFICATION.md')
COVERAGE_VERIFICATION_PROMPT = load_config_file('COVERAGE_VERIFICATION.md')
FUN_FALLBACK_GENERATION_SYSTEM_PROMPT = load_config_file('FUN_FALLBACK_GENERATION_SYSTEM.md')
FUN_FALLBACK_GENERATION_USER_PROMPT = load_config_file('FUN_FALLBACK_GENERATION_USER.md')

def _parse_patterns(text: str) -> List[str]:
    patterns = []
    for line in (text or "").splitlines():
        s = line.strip().lower()
        if not s or s.startswith('#'):
            continue
        patterns.append(s)
    return patterns

# Note: We rely on AI-based fallback classification via `classify_fallback` and
# a lightweight heuristic in `is_fallback_response`. No static pattern list needed.

def _parse_team_rules(text: str) -> Dict[str, List[str]]:
    edu, prog = [], []
    lines = (text or "").splitlines()
    for i, line in enumerate(lines):
        low = line.strip().lower()
        if low.startswith('education team keywords:') and i + 1 < len(lines):
            edu = [w.strip().lower() for w in lines[i+1].split(',') if w.strip()]
        if low.startswith('program team keywords:') and i + 1 < len(lines):
            prog = [w.strip().lower() for w in lines[i+1].split(',') if w.strip()]
    return {"edu": edu, "program": prog}

TEAM_RULES = _parse_team_rules(TEAM_ROUTING_RULES_TEXT)

# Constants for consistent messaging
FALLBACK_MESSAGE = (
    "I don't have complete information about this topic in our curriculum materials. "
    "Please reach out to the Education team on Slack for the specific details you need."
)

PROCESSING_ERROR_MESSAGE = (
    "I'm having trouble processing your request right now. "
    "Please reach out to the Education team on Slack for assistance."
)

# Error classification and retry constants
class ErrorType:
    RETRIEVAL_FAILURE = "retrieval_failure"
    API_RATE_LIMIT = "api_rate_limit"
    VALIDATION_FAILURE = "validation_failure"
    GENERATION_FAILURE = "generation_failure"
    NETWORK_ERROR = "network_error"
    UNKNOWN_ERROR = "unknown_error"

class ErrorSeverity:
    LOW = "low"        # Can retry with degraded service
    MEDIUM = "medium"  # Needs specific recovery strategy
    HIGH = "high"      # Must use fallback immediately

# Retry configuration
MAX_RETRIES = 2
RETRY_DELAY = 1.0  # seconds

# ---------------- Utility Functions ----------------
def normalize_filename(filename: str) -> str:
    if not filename:
        return ""
    base = filename.rsplit("/", 1)[-1]
    base = base.rsplit(".", 1)[0]
    return base.replace("_", " ").strip()


def _build_conversation_context(messages: Sequence[BaseMessage], max_tokens: int = 3000) -> List[Dict[str, str]]:
    """
    Build optimized conversation context with smart truncation.
    
    Args:
        messages: LangGraph managed message history
        max_tokens: Approximate token limit for context (rough estimate: 4 chars = 1 token)
    
    Returns:
        List of formatted messages for OpenAI API
    """
    if not messages:
        return []
    
    conversation_messages = []
    total_chars = 0
    max_chars = max_tokens * 4  # Rough token estimation
    
    # Process messages from most recent backwards
    for msg in reversed(messages):
        if isinstance(msg, HumanMessage):
            role = "user"
            content = msg.content
        elif isinstance(msg, AIMessage):
            role = "assistant"
            # Remove Sources section from assistant messages in context to save space
            content = re.sub(r'\n+Sources:\n(?:- .+\n?)+$', '', msg.content, flags=re.IGNORECASE | re.MULTILINE).strip()
        else:
            continue
        
        # Estimate if adding this message would exceed limit
        message_chars = len(content) + 50  # Add buffer for role/formatting
        if total_chars + message_chars > max_chars and conversation_messages:
            # If we have at least one message, stop here
            break
        
        # Add message to front of list (since we're processing backwards)
        conversation_messages.insert(0, {"role": role, "content": content})
        total_chars += message_chars
        
        # Keep at most 8 messages total for performance
        if len(conversation_messages) >= 8:
            break
    
    logger.info(f"Built conversation context: {len(conversation_messages)} messages, ~{total_chars} chars")
    return conversation_messages

def classify_error(error: Exception) -> tuple[str, str]:
    """
    Classify an error to determine appropriate recovery strategy.
    
    Returns:
        tuple: (error_type, severity)
    """
    error_str = str(error).lower()
    
    # API Rate limits
    if "rate limit" in error_str or "quota" in error_str or "429" in error_str:
        return ErrorType.API_RATE_LIMIT, ErrorSeverity.MEDIUM
    
    # Network/Connection errors
    if any(term in error_str for term in ["connection", "timeout", "network", "502", "503", "504"]):
        return ErrorType.NETWORK_ERROR, ErrorSeverity.MEDIUM
    
    # OpenAI API errors
    if "openai" in error_str and any(term in error_str for term in ["invalid", "api", "unauthorized"]):
        return ErrorType.GENERATION_FAILURE, ErrorSeverity.HIGH
    
    # JSON parsing errors (validation)
    if "json" in error_str or "parse" in error_str:
        return ErrorType.VALIDATION_FAILURE, ErrorSeverity.LOW
    
    # Default classification
    return ErrorType.UNKNOWN_ERROR, ErrorSeverity.MEDIUM

def create_error_record(error: Exception, error_type: str, severity: str, node: str) -> Dict:
    """Create a structured error record for tracking"""
    return {
        "error_type": error_type,
        "severity": severity,
        "node": node,
        "message": str(error),
        "timestamp": time.time()
    }

def should_retry(state: Dict, error_type: str, severity: str) -> bool:
    """Determine if an error should trigger a retry"""
    retry_count = state.get("retry_count", 0)
    
    # Never retry high severity errors
    if severity == ErrorSeverity.HIGH:
        return False
    
    # Don't retry if we've hit max retries
    if retry_count >= MAX_RETRIES:
        return False
    
    # Retry network and rate limit errors
    if error_type in [ErrorType.NETWORK_ERROR, ErrorType.API_RATE_LIMIT]:
        return True
    
    # Retry validation failures (they're often transient)
    if error_type == ErrorType.VALIDATION_FAILURE:
        return True
    
    return False

def ensure_single_sources_block(text: str, filenames: List[str]) -> str:
    text = text or ""
    # Strip existing Sources blocks
    text = re.sub(r'\n+Sources:\n(?:- .+\n?)+$', '', text, flags=re.IGNORECASE | re.MULTILINE)
    if filenames:
        bullets = "\n".join(f"- {normalize_filename(fn)}" for fn in filenames)
        text = f"{text}\n\nSources:\n{bullets}"
    return text.strip()

# ---------------- Coverage Classification (AI-based) ----------------
def classify_coverage(state: 'RAGState') -> 'RAGState':
    """Use AI to determine if the query is a coverage question and extract the topic."""
    query = state.get("query", "")
    try:
        schema = {
            "type": "json_schema",
            "json_schema": {
                "name": "coverage_classifier",
                "schema": {
                    "type": "object",
                    "properties": {
                        "is_coverage_question": {"type": "boolean"},
                        "topic": {"type": "string"}
                    },
                    "required": ["is_coverage_question"]
                }
            }
        }
        prompt = (COVERAGE_CLASSIFICATION_PROMPT or "Classify if this is a coverage question. Return JSON.") + f"\n\nQUERY: {query}"
        resp = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "Return only JSON following the schema."},
                {"role": "user", "content": prompt}
            ],
            temperature=0,
            response_format=schema
        )
        parsed = json.loads(resp.choices[0].message.content)
        is_cov = bool(parsed.get("is_coverage_question", False))
        topic = str(parsed.get("topic", "")).strip()
        logger.info(f"Coverage classification result: is_coverage={is_cov}, topic='{topic}', raw_response='{resp.choices[0].message.content}'")
        return {**state, "is_coverage_question": is_cov, "coverage_topic": topic}
    except Exception as e:
        logger.warning(f"Coverage classification failed, defaulting to not coverage: {e}")
        return {**state, "is_coverage_question": False, "coverage_topic": ""}

# ---------------- Coverage Verification (AI-based) ----------------
def verify_coverage_with_ai(state: 'RAGState') -> 'RAGState':
    """Ask AI if the coverage topic is explicitly listed in the retrieved documents."""
    topic = (state.get("coverage_topic") or "").strip()
    docs: List[str] = state.get("retrieved_docs", [])
    evidence_text = "\n\n".join(docs)[:20000]
    try:
        classification_schema = {
            "type": "json_schema",
            "json_schema": {
                "name": "coverage_presence",
                "schema": {
                    "type": "object",
                    "properties": {
                        "explicitly_listed": {"type": "boolean"},
                        "reason": {"type": "string"}
                    },
                    "required": ["explicitly_listed"]
                }
            }
        }
        base_prompt = COVERAGE_VERIFICATION_PROMPT or "Determine if topic is listed in excerpts. Return JSON."
        prompt = base_prompt.format(topic=topic or 'UNKNOWN', evidence_text=evidence_text)
        ai_resp = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "Return only JSON following the schema."},
                {"role": "user", "content": prompt}
            ],
            temperature=0,
            response_format=classification_schema
        )
        parsed = json.loads(ai_resp.choices[0].message.content)
        explicitly_listed = bool(parsed.get("explicitly_listed", False))
        reason = str(parsed.get("reason", ""))
        logger.info(f"AI coverage verification: {explicitly_listed} ({reason})")
        return {**state, "coverage_explicitly_listed": explicitly_listed, "coverage_verification_reason": reason}
    except Exception as e:
        logger.warning(f"AI coverage verification failed: {e}")
        return {**state, "coverage_explicitly_listed": False, "coverage_verification_reason": "verification_error"}

def route_after_coverage_check(state: 'RAGState') -> str:
    """Route based on AI coverage classification and verification."""
    is_coverage = state.get("is_coverage_question", False)
    explicitly_listed = state.get("coverage_explicitly_listed")
    logger.info(f"Coverage routing: is_coverage={is_coverage}, explicitly_listed={explicitly_listed}")
    # Only route to negative coverage if it's a coverage question and AI verified NOT listed
    if is_coverage and explicitly_listed is False:
        logger.info("Routing to negative coverage response")
        return "generate_negative_coverage_response"
    logger.info("Routing to regular generate_response")
    return "generate_response"

# ---------------- LangGraph State ----------------
class RAGState(TypedDict):
    """Enhanced state for robust RAG workflow with error handling"""
    # Input
    query: str
    conversation_id: str
    
    # Retrieval
    retrieved_docs: List[str]
    sources: List[str]
    selected_file_ids: List[str]
    evidence_chunks: List[Dict]
    
    # Generation
    response: str
    actual_sources_used: List[str]  # Sources actually used by Responses API
    # Structured generation flags
    found_answer_in_documents: bool
    reason_if_not_found: str
    
    # Coverage classification
    is_coverage_question: bool
    coverage_topic: str
    coverage_explicitly_listed: bool
    coverage_verification_reason: str
    
    # Validation
    validation_result: Dict
    confidence: float
    
    # Error handling and recovery
    error_count: int
    last_error: Dict
    retry_count: int
    degraded_mode: bool
    error_history: List[Dict]
    
    # Metadata
    processing_time: float
    retrieved_docs_count: int
    retry_expansion: bool
    
    # LangGraph managed conversation history (this replaces all manual context management!)
    messages: Annotated[Sequence[BaseMessage], add]

# ---------------- Graph Nodes ----------------

def retrieve_documents(state: RAGState) -> RAGState:
    """Retrieve relevant documents using OpenAI Responses API"""
    logger.info(f"Retrieving documents for: {state['query']}")
    
    try:
        # Enhance query with conversation context for better document retrieval
        query = state["query"]
        messages = state.get("messages", [])
        
        # Build context-enhanced query if we have conversation history
        if messages and len(messages) > 1:
            # Get recent conversation context (last 3 messages for context)
            recent_messages = messages[-3:] if len(messages) > 3 else messages[:-1]  # Exclude current query
            
            context_parts = []
            for msg in recent_messages:
                if hasattr(msg, 'content') and msg.content:
                    # Only include the most recent user message for context
                    if isinstance(msg, HumanMessage):
                        context_parts.append(f"Previous context: {msg.content}")
                        break  # Only use the most recent user message for context
            
            if context_parts:
                # Enhance the query with conversation context
                enhanced_query = f"{query}\n\nConversation context: {' '.join(context_parts)}"
                logger.info(f"Enhanced query with context: {enhanced_query[:100]}...")
                query = enhanced_query
        
        instructions = RETRIEVAL_INSTRUCTIONS or ""
        
        resp = openai_client.responses.create(
            model="gpt-4o-mini",
            input=[{"role": "user", "content": query}],
            instructions=instructions,
            tools=[{
                "type": "file_search",
                "vector_store_ids": [VECTOR_STORE_ID],
                "max_num_results": 40
            }],
            tool_choice={"type": "file_search"},
            include=["file_search_call.results"]
        )
        
        # Extract hits (same logic as original)
        hits = []
        for out in getattr(resp, "output", []):
            res = getattr(out, "results", None)
            if res:
                hits = res
                break
            fsc = getattr(out, "file_search_call", None)
            if fsc:
                if getattr(fsc, "results", None):
                    hits = fsc.results
                    break
                if getattr(fsc, "search_results", None):
                    hits = fsc.search_results
                    break
        
        if not hits:
            return {
                **state,
                "retrieved_docs": [],
                "sources": [],
                "selected_file_ids": [],
                "evidence_chunks": [],
                "retrieved_docs_count": 0
            }
        
        # Process hits with program-specific filtering for sales accuracy
        by_file = {}
        
        for r in hits:
            fname = getattr(r, "filename", None) or getattr(getattr(r, "document", None), "filename", None)
            fid = getattr(r, "file_id", None) or getattr(getattr(r, "document", None), "id", None)
            score = float(getattr(r, "score", 0.0) or 0.0)
            
            text = ""
            if hasattr(r, "text") and r.text:
                text = r.text
            else:
                parts = getattr(r, "content", []) or []
                if parts and hasattr(parts[0], "text"):
                    text = parts[0].text
            
            if not fname or not fid or not text or score < 0.0:
                continue
            
            entry = by_file.get(fname)
            if entry is None or score > entry["score"]:
                by_file[fname] = {"text": text, "score": score, "file_id": fid}
        
        # Select top 6 files (increased for AI filtering)
        sorted_files = sorted(by_file.items(), key=lambda kv: kv[1]["score"], reverse=True)
        top_files = sorted_files[:6]
        
        sources = []
        retrieved_content = []
        selected_file_ids = []
        evidence_chunks = []
        
        for fname, entry in top_files:
            sources.append(fname)
            retrieved_content.append(entry["text"])
            selected_file_ids.append(entry["file_id"])
            evidence_chunks.append({
                "file_id": entry["file_id"],
                "filename": fname,
                "text": entry["text"]
            })
        
        return {
            **state,
            "retrieved_docs": retrieved_content,
            "sources": sources,
            "selected_file_ids": selected_file_ids,
            "evidence_chunks": evidence_chunks,
            "retrieved_docs_count": len(retrieved_content)
        }
        
    except Exception as e:
        logger.error(f"Error retrieving documents: {e}")
        
        # Classify error and decide on recovery strategy
        error_type, severity = classify_error(e)
        error_record = create_error_record(e, error_type, severity, "retrieve_documents")
        
        # Update error tracking in state
        error_history = state.get("error_history", [])
        error_history.append(error_record)
        
        updated_state = {
            **state,
            "error_count": state.get("error_count", 0) + 1,
            "last_error": error_record,
            "error_history": error_history
        }
        
        # Check if we should retry
        if should_retry(updated_state, error_type, severity):
            logger.info(f"Scheduling retry for {error_type} error (attempt {updated_state.get('retry_count', 0) + 1})")
            # For this phase, we'll handle retry in the next version
            # For now, use graceful degradation
        
        # Return degraded state
        return {
            **updated_state,
            "retrieved_docs": [],
            "sources": [],
            "selected_file_ids": [],
            "evidence_chunks": [],
            "retrieved_docs_count": 0,
            "degraded_mode": True
        }

def filter_documents_for_sales(state: RAGState) -> RAGState:
    """Use AI to select the most relevant documents for sales accuracy"""
    logger.info("Using AI to select most relevant documents for sales")
    
    query = state["query"]
    original_sources = state["sources"]
    original_docs = state["retrieved_docs"]
    original_file_ids = state["selected_file_ids"]
    original_chunks = state["evidence_chunks"]
    
    # Group chunks by document name to avoid losing relevant chunks from the same document
    chunks_by_doc = {}
    for i, source in enumerate(original_sources):
        doc_name = source.replace('.txt', '').replace('.md', '')
        if doc_name not in chunks_by_doc:
            chunks_by_doc[doc_name] = []
        chunks_by_doc[doc_name].append(i)
    
    # If we have chunks from only 1-2 unique documents, keep them all
    if len(chunks_by_doc) <= 2:
        logger.info(f"Keeping all {len(original_sources)} chunks from {len(chunks_by_doc)} documents")
        return state
    
    try:
        # Create chunk analysis prompt with content previews
        chunk_list = []
        for i, (source, doc_content) in enumerate(zip(original_sources, original_docs)):
            # Get first 200 chars of chunk content as preview
            preview = doc_content[:200].replace('\n', ' ').strip()
            if len(doc_content) > 200:
                preview += "..."
            doc_name = source.replace('.txt', '').replace('.md', '')
            chunk_list.append(f"{i+1}. {doc_name} - \"{preview}\"")
        
        chunk_descriptions = "\n".join(chunk_list)
        
        analysis_prompt = f"""{DOCUMENT_FILTERING_INSTRUCTIONS}

USER QUESTION: {query}

AVAILABLE CONTENT CHUNKS:
{chunk_descriptions}

Return ONLY the numbers of ALL relevant chunks separated by commas (e.g., "1, 3, 4, 6")."""

        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "user", "content": analysis_prompt}
            ],
            temperature=0.1
        )
        
        # Parse the AI response to get selected chunk indices
        ai_response = response.choices[0].message.content.strip()
        logger.info(f"AI chunk selection: {ai_response}")
        
        # Extract numbers from the response
        numbers = re.findall(r'\d+', ai_response)
        selected_chunk_indices = [int(num) - 1 for num in numbers if int(num) <= len(original_sources)]
        
        if selected_chunk_indices:
            filtered_sources = [original_sources[i] for i in selected_chunk_indices]
            filtered_docs = [original_docs[i] for i in selected_chunk_indices]
            filtered_file_ids = [original_file_ids[i] for i in selected_chunk_indices]  
            filtered_chunks = [original_chunks[i] for i in selected_chunk_indices]
            
            # Count selected docs for logging
            selected_doc_names = set()
            for i in selected_chunk_indices:
                doc_name = original_sources[i].replace('.txt', '').replace('.md', '')
                selected_doc_names.add(doc_name)
            
            logger.info(f"AI filtering: {len(original_sources)} chunks → {len(filtered_sources)} chunks from {len(selected_doc_names)} docs")
            for i in selected_chunk_indices:
                preview = original_docs[i][:100].replace('\n', ' ').strip()
                if len(original_docs[i]) > 100:
                    preview += "..."
                logger.info(f"AI selected chunk {i+1}: {original_sources[i]} - \"{preview}\"")
        else:
            # Fallback: keep all chunks from first 2 documents
            logger.warning("AI selection failed, keeping first 2 documents")
            fallback_indices = []
            for doc_name in list(chunks_by_doc.keys())[:2]:
                fallback_indices.extend(chunks_by_doc[doc_name])
            
            filtered_sources = [original_sources[i] for i in fallback_indices]
            filtered_docs = [original_docs[i] for i in fallback_indices]
            filtered_file_ids = [original_file_ids[i] for i in fallback_indices]
            filtered_chunks = [original_chunks[i] for i in fallback_indices]
        
        return {
            **state,
            "sources": filtered_sources,
            "retrieved_docs": filtered_docs,
            "selected_file_ids": filtered_file_ids,
            "evidence_chunks": filtered_chunks,
            "retrieved_docs_count": len(filtered_docs)
        }
        
    except Exception as e:
        logger.error(f"Error in AI document filtering: {e}")
        # Fallback: keep all chunks from first 2 documents
        fallback_indices = []
        for doc_name in list(chunks_by_doc.keys())[:2]:
            fallback_indices.extend(chunks_by_doc[doc_name])
        
        return {
            **state,
            "sources": [original_sources[i] for i in fallback_indices],
            "retrieved_docs": [original_docs[i] for i in fallback_indices],
            "selected_file_ids": [original_file_ids[i] for i in fallback_indices],
            "evidence_chunks": [original_chunks[i] for i in fallback_indices],
            "retrieved_docs_count": len(fallback_indices)
        }

def generate_response(state: RAGState) -> RAGState:
    """Generate response using Chat Completions API with optimized conversation context"""
    logger.info("Generating response")
    
    try:
        # Early exit for AI-verified negative coverage
        if state.get("is_coverage_question", False) and state.get("coverage_explicitly_listed") is False:
            logger.info("Early routing: negative coverage due to AI verification")
            return generate_negative_coverage_response(state)
        if not state["retrieved_docs"]:
            # Generate safe fallback
            return {
                **state,
                "response": FALLBACK_MESSAGE,
                "actual_sources_used": []
            }
        
        # Build instructions for generation
        instructions = f"""{MASTER_PROMPT}

{GENERATION_INSTRUCTIONS}
"""
        
        # Build context from retrieved documents
        context = "\n\n".join(state["retrieved_docs"])
        
        # Create system message with instructions and context
        system_message = f"""{instructions}

RETRIEVED CONTEXT:
{context}
"""
        
        # Build conversation messages - LangGraph automatically manages conversation history
        messages = [{"role": "system", "content": system_message}]
        
        # Add optimized conversation context (smart truncation)
        conversation_context = _build_conversation_context(state.get("messages", []))
        messages.extend(conversation_context)
        
        # Add current query
        messages.append({"role": "user", "content": state["query"]})
        
        # Define structured response schema
        response_schema = {
            "type": "json_schema",
            "json_schema": {
                "name": "rag_response",
                "schema": {
                    "type": "object",
                    "properties": {
                        "answer": {"type": "string"},
                        "found_answer_in_documents": {"type": "boolean"},
                        "reason_if_not_found": {
                            "type": "string",
                            "enum": [
                                "no_relevant_information",
                                "insufficient_detail",
                                "wrong_document_sections"
                            ]
                        }
                    },
                    "required": ["answer", "found_answer_in_documents"]
                }
            }
        }
        
        response = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=messages,
            temperature=0.3,
            response_format=response_schema
        )
        
        structured_response_raw = response.choices[0].message.content
        
        # Parse the structured JSON response
        try:
            structured_response = json.loads(structured_response_raw)
            generated = structured_response.get("answer", "")
            found_answer = structured_response.get("found_answer_in_documents", True)
            reason_if_not_found = structured_response.get("reason_if_not_found", "")
            
            logger.info(f"Structured response - found_answer: {found_answer}, reason: {reason_if_not_found}")
            
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse structured response: {e}")
            # Fallback to treating as plain text
            generated = structured_response_raw
            found_answer = True
            reason_if_not_found = ""
        
        # Smart detection of which sources were actually referenced
        actual_sources_used = []
        response_lower = generated.lower()
        
        for source in state["sources"]:
            # Check for explicit mentions
            if (source.lower() in response_lower or 
                source.replace('.txt', '').lower() in response_lower or
                normalize_filename(source).lower() in response_lower):
                actual_sources_used.append(source)
        
        # If no explicit references found, use the most relevant source (first one)
        if not actual_sources_used and state["sources"]:
            actual_sources_used = [state["sources"][0]]
        
        # Fallback if no content generated
        if not generated:
            generated = FALLBACK_MESSAGE
        
        # Add sources section with only the sources actually used by the API
        if actual_sources_used and "Sources:" not in generated:
            # Remove duplicates while preserving order
            unique_sources = []
            seen = set()
            for source in actual_sources_used:
                if source not in seen:
                    unique_sources.append(source)
                    seen.add(source)
            generated = ensure_single_sources_block(generated, unique_sources)
        
        return {
            **state,
            "response": generated,
            "structured_response": structured_response_raw,
            "found_answer_in_documents": found_answer,
            "reason_if_not_found": reason_if_not_found,
            "actual_sources_used": actual_sources_used
        }
        
    except Exception as e:
        logger.error(f"Error generating response: {e}")
        return {
            **state,
            "response": PROCESSING_ERROR_MESSAGE,
            "actual_sources_used": []
        }

def validate_response(state: RAGState) -> RAGState:
    """Validate the generated response for accuracy"""
    logger.info("Validating response")
    
    try:
        response = state["response"]
        evidence_text = "\n\n".join(state["retrieved_docs"])
        
        if not evidence_text.strip():
            # No evidence to validate against - treat as safe
            validation_result = {
                "contains_only_retrieved_info": True,
                "unsupported_claims": [],
                "confidence": 1.0,
                "explanation": "No evidence available. Treated as safe."
            }
            return {
                **state,
                "validation_result": validation_result,
                "confidence": 1.0
            }
        
        # Strip sources section for validation
        core_resp = re.sub(r'\nSources:\n(?:- .+\n?)+$', '', response, flags=re.IGNORECASE | re.MULTILINE).strip()
        
        validation_instructions = f"""{VALIDATION_INSTRUCTIONS}

EVIDENCE TEXT:
{evidence_text}

RESPONSE TO VALIDATE:
{core_resp}
"""
        
        validation_response = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "Return only valid JSON with the requested keys. No prose."},
                {"role": "user", "content": validation_instructions}
            ],
            temperature=0,
            response_format={"type": "json_object"}
        )
        
        result_raw = json.loads(validation_response.choices[0].message.content.strip())
        validation_result = {
            "contains_only_retrieved_info": bool(result_raw.get("contains_only_retrieved_info", False)),
            "unsupported_claims": [str(x) for x in result_raw.get("unsupported_claims", [])],
            "confidence": float(result_raw.get("confidence", 0)),
            "explanation": str(result_raw.get("explanation", "")),
        }
        
        # Apply softening rules (same as original)
        if (not validation_result["contains_only_retrieved_info"] and
            ((len(validation_result["unsupported_claims"]) <= 2 and validation_result["confidence"] >= 0.3) or
             (len(validation_result["unsupported_claims"]) == 1 and validation_result["confidence"] >= 0.2))):
            validation_result["contains_only_retrieved_info"] = True
            validation_result["unsupported_claims"] = []
            validation_result["explanation"] = "Minor issues ignored for educational content."
            validation_result["confidence"] = max(0.7, validation_result["confidence"])
        
        return {
            **state,
            "validation_result": validation_result,
            "confidence": validation_result["confidence"]
        }
        
    except Exception as e:
        logger.error(f"Error validating response: {e}")
        validation_result = {
            "contains_only_retrieved_info": True,
            "unsupported_claims": [],
            "confidence": 1.0,
            "explanation": "Validator error. Treated as supported."
        }
        return {
            **state,
            "validation_result": validation_result,
            "confidence": 1.0
        }

def expand_document_chunks(state: RAGState) -> RAGState:
    """Expand chunks from existing documents and retry with more content"""
    logger.info("Expanding document chunks for retry generation")
    
    try:
        query = state["query"]
        original_sources = state["sources"]
        
        if not original_sources:
            logger.info("No sources to expand from")
            return state
        
        logger.info(f"Expanding chunks from {len(original_sources)} source documents")
        
        # Get more comprehensive chunks from the same documents
        # Extract suffix from expansion instructions
        default_suffix = " - provide detailed information from these specific documents"
        suffix = default_suffix
        
        if EXPANSION_INSTRUCTIONS_TEXT:
            # Look for the suffix line in the config
            for line in EXPANSION_INSTRUCTIONS_TEXT.splitlines():
                if line.strip().startswith('"') and 'provide detailed information' in line:
                    # Extract the quoted suffix
                    try:
                        suffix = line.strip().strip('"')
                        break
                    except:
                        suffix = default_suffix
        
        enhanced_query = f"{query}{suffix}"
        logger.info(f"Enhanced query for expansion: {enhanced_query[:100]}...")
        
        resp = openai_client.responses.create(
            model="gpt-4o-mini",
            input=[{"role": "user", "content": enhanced_query}],
            instructions=RETRIEVAL_INSTRUCTIONS or "",
            tools=[{
                "type": "file_search",
                "vector_store_ids": [VECTOR_STORE_ID],
                "max_num_results": 20  # Reduced from 60 to prevent timeouts
            }],
            tool_choice={"type": "file_search"},
            include=["file_search_call.results"]
        )
        
        # Extract hits (same logic as original retrieval)
        hits = []
        for out in getattr(resp, "output", []):
            res = getattr(out, "results", None)
            if res:
                hits = res
                break
            fsc = getattr(out, "file_search_call", None)
            if fsc:
                if getattr(fsc, "results", None):
                    hits = fsc.results
                    break
                if getattr(fsc, "search_results", None):
                    hits = fsc.search_results
                    break
        
        if not hits:
            logger.info("No additional hits found - keeping original content")
            return state
        
        # Focus on the same files we already retrieved, but get different/more chunks
        target_files = set(original_sources)
        expanded_chunks = {}
        
        for r in hits:
            fname = getattr(r, "filename", None) or getattr(getattr(r, "document", None), "filename", None)
            fid = getattr(r, "file_id", None) or getattr(getattr(r, "document", None), "id", None)
            score = float(getattr(r, "score", 0.0) or 0.0)
            
            text = ""
            if hasattr(r, "text") and r.text:
                text = r.text
            else:
                parts = getattr(r, "content", []) or []
                if parts and hasattr(parts[0], "text"):
                    text = parts[0].text
            
            if not fname or not fid or not text or score < 0.0:
                continue
            
            # Only keep chunks from our target files
            if fname in target_files:
                if fname not in expanded_chunks:
                    expanded_chunks[fname] = []
                expanded_chunks[fname].append({
                    "text": text,
                    "score": score,
                    "file_id": fid
                })
        
        # Combine original + expanded chunks, prioritizing expanded content
        final_sources = []
        final_docs = []
        final_file_ids = []
        final_evidence_chunks = []
        
        for fname in target_files:
            if fname in expanded_chunks:
                # Sort by score and take the best chunks
                sorted_chunks = sorted(expanded_chunks[fname], key=lambda x: x["score"], reverse=True)
                # Take up to 3 chunks per document to get comprehensive coverage
                for chunk in sorted_chunks[:3]:
                    final_sources.append(fname)
                    final_docs.append(chunk["text"])
                    final_file_ids.append(chunk["file_id"])
                    final_evidence_chunks.append({
                        "file_id": chunk["file_id"],
                        "filename": fname,
                        "text": chunk["text"]
                    })
        
        logger.info(f"Expanded to {len(final_docs)} chunks from {len(set(final_sources))} documents")
        
        return {
            **state,
            "retrieved_docs": final_docs,
            "sources": final_sources,
            "selected_file_ids": final_file_ids,
            "evidence_chunks": final_evidence_chunks,
            "retrieved_docs_count": len(final_docs),
            "retry_expansion": True  # Flag to indicate this is an expanded retry
        }
        
    except Exception as e:
        logger.error(f"Error expanding document chunks: {e}")
        # Return original state if expansion fails
        return state

def generate_fun_fallback(state: RAGState) -> RAGState:
    """Generate a fun, personality-driven fallback response"""
    logger.info("Generating fun fallback response")
    
    query = state.get("query", "")
    found_answer = state.get("found_answer_in_documents", True)
    reason_if_not_found = state.get("reason_if_not_found", "")
    
    # Categorize the query to determine which team to suggest
    query_lower = query.lower()
    
    # Content/curriculum/certification queries → Education team
    # Load keywords from config; fall back to defaults
    edu_keywords = TEAM_RULES.get("edu") or [
        "curriculum", "course", "content", "syllabus", "certification", "taught", "covered", "learn", "study", "technologies", "tools", "languages", "duration", "hours", "weeks"
    ]
    
    # Operations/logistics/process queries → Program team  
    program_keywords = TEAM_RULES.get("program") or [
        "schedule", "start", "when", "application", "apply", "price", "cost", "payment", "location", "format", "requirements", "prerequisites", "job", "placement", "career"
    ]
    
    is_edu_query = any(keyword in query_lower for keyword in edu_keywords)
    is_program_query = any(keyword in query_lower for keyword in program_keywords)
    
    # Choose team and generate fun response
    if is_edu_query:
        team = "Education team"
        team_context = "They know all the curriculum secrets!"
    elif is_program_query:
        team = "Program team (the ones actually running these courses)"
        team_context = "They handle all the logistics and can give you the real scoop!"
    else:
        team = "Education team"
        team_context = "When in doubt, they're your best bet!"
    
    # Generate fun responses based on reason
    if not found_answer:
        if reason_if_not_found == "insufficient_detail":
            personality_intro = "🤔 I found some info but not enough to give you the full picture (don't want to hurt Rudy's feelings with incomplete answers!)"
        elif reason_if_not_found == "no_relevant_information":
            personality_intro = "🕵️ I searched high and low through our docs but came up empty-handed - better to admit defeat than start a fight with the Product team about missing documentation!"
        elif reason_if_not_found == "wrong_document_sections":
            personality_intro = "📚 I might be looking in the wrong sections of our docs (happens to the best of us!)"
        else:
            personality_intro = "🤷 I'm playing it safe here rather than risk giving you dodgy information"
    else:
        # This shouldn't happen but let's handle it gracefully
        personality_intro = "🛡️ I'm being extra cautious with this one"
    
    # Try AI-crafted fun fallback using guardrails and templates
    fun_response = None
    try:
        templates_text = FUN_FALLBACK_TEMPLATES or ""
        system_prompt = FUN_FALLBACK_GENERATION_SYSTEM_PROMPT or "Generate a friendly fallback message."
        user_prompt_template = FUN_FALLBACK_GENERATION_USER_PROMPT or "Craft a fallback for query: {query}"
        user_prompt = user_prompt_template.format(
            query=query,
            reason=reason_if_not_found or 'unspecified',
            team=team,
            team_context=team_context,
            templates_text=templates_text
        )
        ai = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.75,
        )
        fun_response = (ai.choices[0].message.content or "").strip()
    except Exception:
        fun_response = None

    # If AI generation fails or returns empty, fall back to a neutral processing message
    if not fun_response:
        fun_response = PROCESSING_ERROR_MESSAGE
    
    fallback_validation = {
        "contains_only_retrieved_info": True,
        "unsupported_claims": [],
        "confidence": 1.0,
        "explanation": "Used fun fallback response with appropriate team routing."
    }
    
    return {
        **state,
        "response": fun_response,
        "validation_result": fallback_validation,
        "confidence": 1.0
    }

def generate_negative_coverage_response(state: 'RAGState') -> 'RAGState':
    """Generate a concise negative coverage answer based on absence in retrieved curriculum."""
    logger.info("Generating negative coverage response")
    query = state.get("query", "")
    topic = (state.get("coverage_topic") or "the requested topic").strip() or "the requested topic"
    # Choose sources as-is; avoid hardcoded bias. Future: AI-based selection.
    sources = state.get("sources", [])
    chosen_sources = sources
    filenames = [normalize_filename(s) for s in chosen_sources]
    cited = filenames[0] if filenames else "retrieved curriculum"
    answer = f"No — according to the retrieved curriculum, {topic} is not listed."
    # Add a single Sources block
    answer = ensure_single_sources_block(answer, chosen_sources)
    validation = {
        "contains_only_retrieved_info": True,
        "unsupported_claims": [],
        "confidence": 0.9,
        "explanation": f"Answered based on absence of explicit mention in {cited}."
    }
    return {
        **state,
        "response": answer,
        "validation_result": validation,
        "confidence": 0.9
    }

def apply_fallback(state: RAGState) -> RAGState:
    """Apply safe fallback if validation fails"""
    logger.info("Applying fallback due to validation failure")
    
    # Use the fun fallback generator
    return generate_fun_fallback(state)

def finalize_response(state: RAGState) -> RAGState:
    """Add assistant response to conversation history and finalize state"""
    # Add the assistant's response to the conversation history for future context
    if state.get("response"):
        # Create a new messages list with the assistant's response added
        current_messages = list(state.get("messages", []))
        current_messages.append(AIMessage(content=state["response"]))
        
        return {
            **state,
            "messages": current_messages
        }
    
    return state

# ---------------- Error Recovery Nodes ----------------

def handle_retrieval_error(state: RAGState) -> RAGState:
    """Handle retrieval failures with degraded service"""
    logger.warning("Handling retrieval error with degraded service")
    
    return {
        **state,
        "retrieved_docs": [],
        "sources": [],
        "selected_file_ids": [],
        "evidence_chunks": [],
        "retrieved_docs_count": 0,
        "degraded_mode": True,
        "retry_count": 0  # Reset for next stage
    }

def handle_generation_error(state: RAGState) -> RAGState:
    """Handle generation failures with simplified approach"""
    logger.warning("Handling generation error with simplified approach")
    
    # Try simple template-based response
    query_lower = state.get("query", "").lower()
    
    if any(word in query_lower for word in ["technolog", "tools", "software"]):
        simplified_response = (
            "I'd be happy to help you learn about the technologies covered in our programs. "
            "Please reach out to the Education team on Slack for detailed information about "
            "specific tools and technologies used in each bootcamp."
        )
    elif any(word in query_lower for word in ["duration", "time", "long", "hours"]):
        simplified_response = (
            "Our bootcamps vary in duration depending on the program format. "
            "Please reach out to the Education team on Slack for specific timing "
            "information about the program you're interested in."
        )
    else:
        simplified_response = FALLBACK_MESSAGE
    
    return {
        **state,
        "response": simplified_response,
        "actual_sources_used": [],
        "degraded_mode": True,
        "retry_count": 0  # Reset for next stage
    }

def handle_validation_error(state: RAGState) -> RAGState:
    """Handle validation failures by accepting response with warning"""
    logger.warning("Handling validation error - accepting response with reduced confidence")
    
    # Create a permissive validation result
    validation_result = {
        "contains_only_retrieved_info": True,  # Accept the response
        "unsupported_claims": [],
        "confidence": 0.5,  # Reduced confidence due to validation failure
        "explanation": "Validation system failed, accepted with reduced confidence."
    }
    
    return {
        **state,
        "validation_result": validation_result,
        "confidence": 0.5,
        "degraded_mode": True
    }

def retry_with_delay(state: RAGState) -> RAGState:
    """Add delay before retry for rate limiting and network issues"""
    error_type = state.get("last_error", {}).get("error_type", "")
    
    if error_type == ErrorType.API_RATE_LIMIT:
        delay = RETRY_DELAY * (state.get("retry_count", 0) + 1) * 2  # Exponential backoff
        logger.info(f"Rate limit hit, waiting {delay} seconds before retry")
        time.sleep(delay)
    elif error_type == ErrorType.NETWORK_ERROR:
        delay = RETRY_DELAY
        logger.info(f"Network error, waiting {delay} seconds before retry")
        time.sleep(delay)
    
    # Increment retry count
    return {
        **state,
        "retry_count": state.get("retry_count", 0) + 1
    }

# ---------------- Conditional Logic ----------------

def should_expand_chunks(state: 'RAGState') -> str:
    """Determine if we should expand chunks and retry generation using structured/AI fallback flags"""
    response = state.get("response", "")
    retry_expansion = state.get("retry_expansion", False)
    sources = state.get("sources", [])
    found_answer = state.get("found_answer_in_documents", True)
    reason_if_not_found = state.get("reason_if_not_found", "")
    is_fallback_ai = state.get("is_fallback_ai", None)
    
    logger.info(f"Checking for chunk expansion: response_length={len(response)}, has_sources={bool(sources)}, already_tried={retry_expansion}")
    logger.info(f"Raw state keys: {list(state.keys())}")
    logger.info(f"found_answer_in_documents in state: {state.get('found_answer_in_documents', 'NOT FOUND')}")
    logger.info(f"Structured response analysis: found_answer={found_answer}, reason={reason_if_not_found}")
    # If AI coverage verification already determined it's not listed, route negative
    if state.get("is_coverage_question", False) and state.get("coverage_explicitly_listed") is False:
        logger.info("AI coverage verification says not listed; generating negative coverage response")
        return "generate_negative_coverage_response"
    # Prefer AI node-driven classification; avoid inline detection to keep logic centralized in the node
    legacy_fallback = bool(is_fallback_ai)
    logger.info(f"Using AI node classification for fallback: {legacy_fallback}")
    
    # Do NOT force negative coverage if AI said explicitly listed (True or None). Only negative when explicitly False.
    # If coverage question and not found, prefer expansion first; only negative when explicitly False.
    
    # Expand if model said not found or legacy fallback text produced, once
    if ((legacy_fallback or not found_answer) and sources and not retry_expansion):
        logger.info("Triggering expansion due to missing answer/legacy fallback")
        return "expand_document_chunks"
    
    # If we tried expansion and still no answer (or still legacy fallback), prefer negative coverage if applicable, else fun fallback
    if (retry_expansion and (legacy_fallback or not found_answer)):
        if state.get("is_coverage_question", False) and sources:
            logger.info("Post-expansion: still missing; generating negative coverage response")
            return "generate_negative_coverage_response"
        logger.info("Expansion unsuccessful; routing to fun fallback")
        return "generate_fun_fallback"
    
    logger.info("Model found answer or no expansion needed - proceeding to validation")
    return "validate_response"

def classify_fallback(state: RAGState) -> RAGState:
    """Classify if current response is a fallback using AI (node-only, no inline heuristics)."""
    response = state.get("response", "")
    found_answer = state.get("found_answer_in_documents", True)
    # Only run when structured flags say not found or when response is short/empty
    should_run = (not found_answer) or (len(response.strip()) < 20)
    if not should_run:
        return state
    try:
        classification_schema = {
            "type": "json_schema",
            "json_schema": {
                "name": "fallback_classifier",
                "schema": {
                    "type": "object",
                    "properties": {
                        "is_fallback": {"type": "boolean"},
                        "reason": {"type": "string"}
                    },
                    "required": ["is_fallback"]
                }
            }
        }
        prompt = (FALLBACK_CLASSIFIER_PROMPT or "Classify if response is a fallback. Return JSON.") + f"\n\nRESPONSE:\n{response}"
        ai_resp = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "Return only JSON following the schema."},
                {"role": "user", "content": prompt}
            ],
            temperature=0,
            response_format=classification_schema
        )
        parsed = json.loads(ai_resp.choices[0].message.content)
        is_fb = bool(parsed.get("is_fallback", False))
        reason = str(parsed.get("reason", ""))
        logger.info(f"AI fallback classification node: {is_fb} ({reason})")
        return {**state, "is_fallback_ai": is_fb, "fallback_reason_ai": reason}
    except Exception as e:
        logger.warning(f"AI fallback classification node failed, using heuristic: {e}")
        heuristic_flag = is_fallback_response(response)
        return {**state, "is_fallback_ai": heuristic_flag, "fallback_reason_ai": "heuristic"}

def should_apply_fallback(state: RAGState) -> str:
    """Determine if we should apply fallback based on validation"""
    confidence_threshold = 0.4
    validation = state.get("validation_result", {})
    
    confidence = validation.get('confidence', 0)
    contains_only_retrieved = validation.get('contains_only_retrieved_info', False)
    
    logger.info(f"Fallback decision: confidence={confidence}, contains_only_retrieved={contains_only_retrieved}, threshold={confidence_threshold}")
    
    if (confidence < confidence_threshold or not contains_only_retrieved):
        logger.info(f"APPLYING FALLBACK: confidence {confidence} < {confidence_threshold} OR contains_only_retrieved={contains_only_retrieved}")
        return "apply_fallback"
    
    logger.info("FINALIZING: validation passed")
    return "finalize"

def should_retry_retrieval(state: RAGState) -> str:
    """Determine if retrieval should be retried based on error"""
    last_error = state.get("last_error", {})
    error_type = last_error.get("error_type", "")
    severity = last_error.get("severity", "")
    
    if should_retry(state, error_type, severity):
        return "retry_with_delay"
    else:
        return "handle_retrieval_error"

def should_retry_generation(state: RAGState) -> str:
    """Determine if generation should be retried based on error"""
    last_error = state.get("last_error", {})
    error_type = last_error.get("error_type", "")
    severity = last_error.get("severity", "")
    
    if should_retry(state, error_type, severity):
        return "retry_with_delay"
    else:
        return "handle_generation_error"

def should_retry_validation(state: RAGState) -> str:
    """Determine if validation should be retried based on error"""
    last_error = state.get("last_error", {})
    error_type = last_error.get("error_type", "")
    severity = last_error.get("severity", "")
    
    if should_retry(state, error_type, severity):
        return "retry_with_delay"
    else:
        return "handle_validation_error"

# ---------------- Graph Construction ----------------
def create_rag_graph():
    """Create the robust LangGraph workflow with error handling"""
    workflow = StateGraph(RAGState)
    
    # Add main workflow nodes
    workflow.add_node("retrieve_documents", retrieve_documents)
    workflow.add_node("filter_documents_for_sales", filter_documents_for_sales)
    workflow.add_node("classify_coverage", classify_coverage)
    workflow.add_node("verify_coverage_with_ai", verify_coverage_with_ai)
    workflow.add_node("generate_response", generate_response)
    workflow.add_node("classify_fallback", classify_fallback)
    workflow.add_node("expand_document_chunks", expand_document_chunks)
    workflow.add_node("generate_fun_fallback", generate_fun_fallback)
    workflow.add_node("validate_response", validate_response)
    workflow.add_node("generate_negative_coverage_response", generate_negative_coverage_response)
    workflow.add_node("apply_fallback", apply_fallback)
    workflow.add_node("finalize_response", finalize_response)
    
    # Add error recovery nodes
    workflow.add_node("handle_retrieval_error", handle_retrieval_error)
    workflow.add_node("handle_generation_error", handle_generation_error)
    workflow.add_node("handle_validation_error", handle_validation_error)
    workflow.add_node("retry_with_delay", retry_with_delay)
    
    # Define the main flow
    workflow.set_entry_point("retrieve_documents")
    workflow.add_edge("retrieve_documents", "filter_documents_for_sales")
    workflow.add_edge("filter_documents_for_sales", "classify_coverage")
    # If it's a coverage question, verify presence before generating
    workflow.add_conditional_edges(
        "classify_coverage",
        lambda s: "verify_coverage_with_ai" if s.get("is_coverage_question", False) else "generate_response",
        {
            "verify_coverage_with_ai": "verify_coverage_with_ai",
            "generate_response": "generate_response",
        }
    )
    workflow.add_conditional_edges(
        "verify_coverage_with_ai",
        route_after_coverage_check,
        {
            "generate_negative_coverage_response": "generate_negative_coverage_response",
            "generate_response": "generate_response",
        }
    )
    
    # After generation, classify fallback when likely needed
    workflow.add_edge("generate_response", "classify_fallback")
    # Add conditional routing after classification - check for expansion/fallback
    workflow.add_conditional_edges(
        "classify_fallback",
        should_expand_chunks,
        {
            "expand_document_chunks": "expand_document_chunks",
            "generate_negative_coverage_response": "generate_negative_coverage_response",
            "generate_fun_fallback": "generate_fun_fallback",
            "validate_response": "validate_response"
        }
    )
    
    # After expansion, generate again and validate
    workflow.add_edge("expand_document_chunks", "generate_response")
    
    # Conditional routing after validation (original logic)
    workflow.add_conditional_edges(
        "validate_response",
        should_apply_fallback,
        {
            "apply_fallback": "apply_fallback",
            "finalize": "finalize_response"
        }
    )
    
    # Error recovery routing - these will be triggered by enhanced nodes
    # (Note: Error routing is handled within nodes for now, but could be expanded)
    
    # Standard completion
    workflow.add_edge("apply_fallback", "finalize_response")
    workflow.add_edge("generate_fun_fallback", "finalize_response")
    workflow.add_edge("finalize_response", END)
    
    # Error recovery completion paths
    workflow.add_edge("handle_retrieval_error", "generate_response")
    workflow.add_edge("handle_generation_error", "finalize_response") 
    workflow.add_edge("handle_validation_error", "finalize_response")
    
    # Retry routing - retry goes back to the failed node
    workflow.add_edge("retry_with_delay", "retrieve_documents")  # Could be made more specific
    
    # Set up memory for persistent conversation state
    memory = MemorySaver()
    
    return workflow.compile(checkpointer=memory)

# Initialize the simplified graph
rag_graph = create_rag_graph()

# ---------------- Slack Integration ----------------
def clean_citations(text):
    text = re.sub(r'\[([^\]]+)\]\([^)]+\)', r'\1', text)  # [text](url) -> text
    text = re.sub(r'\[([^\]]+)\]', r'\1', text)           # [text] -> text
    return text

def convert_markdown_to_slack(text):
    text = re.sub(r'\*\*(.*?)\*\*', r'*\1*', text)  # bold
    text = re.sub(r'(?<!\*)\*([^*]+)\*(?!\*)', r'_\1_', text)  # italics
    text = re.sub(r'^#+\s*(.+)$', r'*\1*', text, flags=re.MULTILINE)  # headers
    text = re.sub(r'^-\s+', '• ', text, flags=re.MULTILINE)  # bullets
    return text

# Ignore Slack events that are edits/changes or from bots to avoid loops
def _is_ignorable_slack_event(event: Dict) -> bool:
    try:
        subtype = event.get('subtype')
        if subtype in {"message_changed", "message_deleted", "message_replied", "bot_message"}:
            return True
        if event.get('bot_id'):
            return True
        nested = event.get('message') or {}
        if isinstance(nested, dict):
            nsub = nested.get('subtype')
            if nsub in {"message_changed", "message_deleted", "message_replied", "bot_message"}:
                return True
            if nested.get('bot_id'):
                return True
    except Exception:
        return False
    return False

# Progressive Slack updates manager
class SlackUpdateManager:
    def __init__(self, client, channel: str, thread_ts: str | None):
        self.client = client
        self.channel = channel
        self.thread_ts = thread_ts
        self.ts = None  # message timestamp for updates
        self.last_update = 0.0
        # Step states: todo | in_progress | done
        self.steps = {
            "Retrieval": "in_progress",
            "Filtering": "todo",
            "Coverage check": "todo",
            "Generation": "todo",
            "Validation": "todo",
        }
        self.draft_text = None

    def _status_emoji(self, state: str) -> str:
        return {
            "todo": "▫️",
            "in_progress": "⏳",
            "done": "✅",
        }.get(state, "▫️")

    def _render_text(self) -> str:
        checklist_lines = []
        for name, st in self.steps.items():
            checklist_lines.append(f"{self._status_emoji(st)} {name}")
        checklist = "\n".join(checklist_lines)

        header = "✨ On it! I’ll keep you posted step-by-step. 🎯"
        body = header + "\n\n" + checklist
        if self.draft_text:
            body += "\n\n*Draft (will improve as we go):*\n" + self.draft_text
        return convert_markdown_to_slack(body)

    def _maybe_throttle(self):
        now = time.time()
        if now - self.last_update < 1.0:
            return True
        self.last_update = now
        return False

    def post_initial(self):
        try:
            if not self.client:
                return
            text = self._render_text()
            resp = self.client.chat_postMessage(channel=self.channel, text=text, thread_ts=self.thread_ts)
            self.ts = resp.get("ts") if isinstance(resp, dict) else getattr(resp, "data", {}).get("ts", None)
        except Exception as e:
            logger.warning(f"Failed to post initial Slack message: {e}")

    def update_step(self, name: str, state: str):
        if name in self.steps:
            self.steps[name] = state
        # Avoid spamming Slack
        if self._maybe_throttle():
            return
        self._update()

    def set_draft(self, text: str):
        # Clean and trim draft for Slack
        cleaned = clean_citations(text or "").strip()
        # Keep it short for draft stage
        if len(cleaned) > 900:
            cleaned = cleaned[:900].rstrip() + "…"
        self.draft_text = cleaned
        if self._maybe_throttle():
            return
        self._update()

    def finalize(self, final_text: str):
        try:
            for k in self.steps.keys():
                self.steps[k] = "done"
            self.draft_text = None
            text = convert_markdown_to_slack(clean_citations(final_text or ""))
            if self.client and self.ts:
                self.client.chat_update(channel=self.channel, ts=self.ts, text=text)
        except Exception as e:
            logger.warning(f"Failed to finalize Slack message: {e}")

    def error(self, message: str):
        try:
            for k in self.steps.keys():
                if self.steps[k] != "done":
                    self.steps[k] = "todo"
            text = convert_markdown_to_slack(f"😅 Little hiccup on my end — {message} \nI’ll keep things simple and share what I can. 🛟")
            if self.client:
                if self.ts:
                    self.client.chat_update(channel=self.channel, ts=self.ts, text=text)
                else:
                    self.client.chat_postMessage(channel=self.channel, text=text, thread_ts=self.thread_ts)
        except Exception as e:
            logger.warning(f"Failed to send error Slack message: {e}")

    def _update(self):
        if not (self.client and self.ts):
            return
        try:
            self.client.chat_update(channel=self.channel, ts=self.ts, text=self._render_text())
        except Exception as e:
            logger.debug(f"Slack update throttled/failed: {e}")

# Initialize Slack app
slack_app = None
handler = None
try:
    if SLACK_BOT_TOKEN and SLACK_SIGNING_SECRET:
        slack_app = App(token=SLACK_BOT_TOKEN, signing_secret=SLACK_SIGNING_SECRET)
        handler = SlackRequestHandler(slack_app)
        logger.info("Slack app initialized successfully")
    else:
        logger.info("Slack env vars missing. Running without Slack handlers.")
except Exception as e:
    logger.error(f"Failed to initialize Slack app: {e}")
    slack_app = None
    handler = None

def process_message(event, say):
    """Process incoming Slack message using LangGraph with optimized conversation handling"""
    user_message = event.get('text', '')
    
    # Determine thread context for proper Slack threading
    thread_ts = None
    conversation_id = None
    
    if event.get('thread_ts'):
        # This is a reply in an existing thread
        thread_ts = event['thread_ts']  # Reply to the original thread
        conversation_id = f"thread_{event['thread_ts']}"
        logger.info(f"Replying in existing thread: {thread_ts}")
    else:
        # This is a new mention/message - create a new thread using this message's timestamp
        thread_ts = event.get('ts')  # Start new thread with this message
        if event.get('channel'):
            conversation_id = f"thread_{event.get('ts')}"  # Use thread-based ID even for new threads
        else:
            conversation_id = f"fallback_{hash(user_message)}_{int(time.time())}"
        logger.info(f"Starting new thread: {thread_ts}")
    
    # Fallback for edge cases
    if not conversation_id:
        conversation_id = f"fallback_{hash(user_message)}_{int(time.time())}"
    
    logger.info(f"Processing message in conversation: {conversation_id}")
    logger.info(f"User message: {user_message[:100]}{'...' if len(user_message) > 100 else ''}")
    
    try:
        start_time = time.time()
        
        # Create initial state - let LangGraph handle conversation history automatically
        initial_state = {
            "query": user_message,
            "conversation_id": conversation_id,
            "messages": [HumanMessage(content=user_message)],  # Current message only
            "processing_time": 0.0,
            # Initialize error handling fields
            "error_count": 0,
            "last_error": {},
            "retry_count": 0,
            "degraded_mode": False,
            "error_history": []
        }
        
        # Prepare Slack progressive updates (no draft content, only status)
        channel = event.get('channel')
        manager = SlackUpdateManager(slack_app.client if slack_app else None, channel, thread_ts)
        manager.post_initial()

        # Stream node updates for progress (fun + emojis, no draft content)
        config = {"configurable": {"thread_id": conversation_id}}
        streamed_result = None
        try:
            stream_iter = getattr(rag_graph, "stream", None)
            if stream_iter is None:
                raise RuntimeError("stream not available")
            for step in stream_iter(initial_state, config=config):
                if not isinstance(step, dict):
                    continue
                for node_name, node_value in step.items():
                    if node_name == "__start__":
                        continue
                    if node_name == "__end__":
                        streamed_result = node_value
                        continue
                    n = str(node_name)
                    if "retrieve_documents" in n:
                        manager.update_step("Retrieval", "done")  # 🔎
                        manager.update_step("Filtering", "in_progress")  # 🧹
                    elif "filter_documents_for_sales" in n:
                        manager.update_step("Filtering", "done")
                        manager.update_step("Coverage check", "in_progress")  # 🧭
                    elif "classify_coverage" in n or "verify_coverage_with_ai" in n:
                        manager.update_step("Coverage check", "done")
                        manager.update_step("Generation", "in_progress")  # ✍️
                    elif ("generate_response" in n or 
                          "generate_negative_coverage_response" in n or 
                          "generate_fun_fallback" in n):
                        manager.update_step("Generation", "done")
                        manager.update_step("Validation", "in_progress")  # 🔎
                    elif "validate_response" in n:
                        manager.update_step("Validation", "done")  # ✅

            # Use streamed final state if available
            if streamed_result is not None:
                result = streamed_result
            else:
                result = rag_graph.invoke(initial_state, config=config)
        except Exception as stream_err:
            logger.info(f"Stream not available or failed, falling back to invoke: {stream_err}")
            result = rag_graph.invoke(initial_state, config=config)
        
        # Calculate processing time
        processing_time = time.time() - start_time
        
        assistant_message = result["response"]
        
        logger.info("Enhanced LangGraph RAG Results:")
        logger.info(f"   Retrieved docs: {result.get('retrieved_docs_count', 0)}")
        logger.info(f"   Sources: {result.get('sources', [])}")
        logger.info(f"   Validation confidence: {result.get('confidence', 0):.2f}")
        logger.info(f"   Processing time: {processing_time:.2f}s")
        logger.info(f"   Error count: {result.get('error_count', 0)}")
        logger.info(f"   Degraded mode: {result.get('degraded_mode', False)}")
        
        # Log error history if any errors occurred
        error_history = result.get('error_history', [])
        if error_history:
            logger.warning(f"Errors during processing: {len(error_history)} total")
            for i, error in enumerate(error_history):
                logger.warning(f"   Error {i+1}: {error.get('error_type', 'unknown')} in {error.get('node', 'unknown')}")
        
        # Finalize the single Slack message with the answer
        manager.finalize(assistant_message)
        logger.info(f"Response finalized successfully in thread: {thread_ts}")
        
    except Exception as e:
        logger.error(f"Error processing message with Simplified LangGraph RAG: {str(e)}")
        try:
            # Update the progress message instead of posting a new one
            channel = event.get('channel')
            manager = SlackUpdateManager(slack_app.client if slack_app else None, channel, thread_ts)
            manager.error(PROCESSING_ERROR_MESSAGE)
        except Exception as e2:
            logger.error(f"Error sending failure response: {str(e2)}")

# Register Slack handlers
if slack_app is not None:
    @slack_app.event("app_mention")
    def handle_mention(event, say):
        """Handle @productwizard mentions in channels"""
        if _is_ignorable_slack_event(event):
            logger.info("Ignoring ignorable app_mention event (bot/edit)")
            return
        logger.info(f"App mention detected in channel {event.get('channel')}, thread_ts: {event.get('thread_ts')}")
        process_message(event, say)

    @slack_app.event("message")
    def handle_message(event, say):
        """Handle direct messages and thread replies where the bot was previously mentioned"""
        if _is_ignorable_slack_event(event):
            logger.info("Ignoring ignorable message event (bot/edit)")
            return
        # Handle direct messages
        if event.get("channel_type") == "im":
            logger.info(f"Direct message received from user")
            process_message(event, say)
        # Handle replies in threads where we've already participated
        elif event.get("thread_ts") and not event.get("bot_id"):
            # This is a human message in a thread - check if we should respond
            # Note: We rely on the conversation memory to determine if we're part of this thread
            logger.info(f"Thread reply detected in thread {event.get('thread_ts')}")
            process_message(event, say)

# ---------------- Flask App ----------------
flask_app = Flask(__name__)

@flask_app.route("/slack/events", methods=["POST"])
def slack_events():
    if handler:
        try:
            return handler.handle(request)
        except Exception as e:
            logger.error(f"Error handling Slack event: {e}")
            return {"error": f"Error processing Slack event: {str(e)}"}, 500
    else:
        logger.error("Slack handler not initialized - check environment variables")
        return {"error": "Slack handler not initialized - check SLACK_BOT_TOKEN and SLACK_SIGNING_SECRET"}, 500

@flask_app.route("/health", methods=["GET"])
def health_check():
    return {
        "status": "healthy",
        "api": "langgraph_rag",
        "pipeline": "retrieve_ai_filter_generate_validate",
        "validation": "enabled",
        "memory_management": "automatic_via_langgraph",
        "document_filtering": "ai_powered_sales_accuracy",
        "architecture": "clean_configurable_instructions"
    }

if __name__ == "__main__":
    flask_app.run(debug=True, host='0.0.0.0', port=int(os.environ.get('PORT', 5000)))
